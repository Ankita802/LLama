{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Basic Prompt Summarization\\n2. PromptTemplate Text Summarization\\n3. StuffDocumentChain Text Summarization\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Basic Prompt Summarization\n",
    "2. PromptTemplate Text Summarization\n",
    "3. StuffDocumentChain Text Summarization\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt Template Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the speech text\n",
    "#speech = \"This is the text of the speech.\"\n",
    "\n",
    "\n",
    "generic_template='''\n",
    "Write a summary of the following speech:\n",
    "Speech : `{speech}`\n",
    "Translate the precise summary to {language}.'''\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    input_variables=[\"speech\",'language'],\n",
    "    template=generic_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWrite a summary of the following speech:\\nSpeech : `This is the text of the speech.`\\nTranslate the precise summary to Hindi.'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(speech=\"This is the text of the speech.\",language='Hindi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Format the prompt with the speech and language\n",
    "# formatted_prompt = prompt.format(speech=speech, language='Hindi')\n",
    "# print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import GooglePalm\n",
    "llm = GooglePalm(google_api_key = 'AIzaSyBkIxznvzMOl84OIaSEU6YOhQXNNlmuDAg',  temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# llm.get_num_tokens(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if speech and language are defined\n",
    "# speech = \"This is the text of the speech.\"\n",
    "# language = 'Hindi'\n",
    "\n",
    "# Define the input dictionary\n",
    "# input_dict = {\"speech\": \"This is the text of the speech.\" , \"language\": \"HINDI\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_dict = {'speech': \"This is the text of the speech.\" , 'language': 'Hindi'}\n",
    "# llm_chain = LLMChain(llm=llm, prompt = prompt)\n",
    "\n",
    "# llm_chain({'speech': \"This is the text of the speech\" , 'language': \"Hindi\"})\n",
    "\n",
    "# # Correct the language value to lowercase\n",
    "# input_dict = {\"speech\": \"This is the text of the speech.\", \"language\": \"hindi\"}\n",
    "\n",
    "# # Create the LLMChain object with the corrected input\n",
    "# llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# # Run the LLMChain\n",
    "# try:\n",
    "#     summary = llm_chain.run(input_dict)\n",
    "#     print(\"Summary:\", summary)\n",
    "# except Exception as e:\n",
    "#     print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\ankita.ankita\\appdata\\roaming\\python\\python39\\site-packages (from PyPDF2) (4.9.0)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/232.6 kB ? eta -:--:--\n",
      "   ------------ -------------------------- 71.7/232.6 kB 787.7 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 143.4/232.6 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 225.3/232.6 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 225.3/232.6 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 225.3/232.6 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- 232.6/232.6 kB 839.0 kB/s eta 0:00:00\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StuffDocumentChain Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "# obj = PdfFileReader('abc.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfreader = PdfReader('1822-b.e-cse-batchno-224.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "# read text from pdf \n",
    "text = ''\n",
    "for i , page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 \\n  \\nFINAL YEAR PROJECT REPORT  \\nat \\nSathyabama Institute of Science and Technology (Deemed to be \\nUniversity)  \\n \\nSubmitted in partial fulfilment of the requirements for the award of Bachelor of \\nEngineering Degree in Computer Science and Engineering  \\n \\n \\n \\n                                                              Pitambara Awadhesh  \\n(Reg. No. 38110406)  \\nReema Rose Toppo  \\n(Reg. No. 38110458)  \\n \\n \\n \\n \\nDEPARTMENT OF COMPUTER SCIENCE AND ENGINEERING SCHOOL OF \\nCOMPUTING  \\nSATHYABAMA INSTITUTE OF SCIENCE AND TECHNOLOGY JEPPIAAR NAGAR, \\nRAJIV GANDHI SALAI,  \\nCHENNAI – 600119, TAMILNADU  \\n2 \\n 3 \\n                                    SATHYABAMA  \\n                                                           INSTITUTE OF SCIENCE AND TECHNOLOGY  \\n(DEEMED TO BE UNIVERSITY)  \\n                                                 Accredited with Grade “A” by NAAC  \\n                                                        (Established under Section 3 of UGC Act, 1956)  \\nJEPPIAAR NAGAR, RAJIV GANDHI SALAI, CHENNAI – 600119  \\nwww.sathyabamauniversity.ac.in  \\n \\n_________________________________________________________________________________  \\nDEPARTMENT OF COMPUTER SCIENCE AND ENGINEERING  \\n \\nBONAFIDE CERTIFICATE  \\n \\nThis is to certify that this Project Report is the bonafide work of Pitambara Awadhesh (38110406) and \\nReema Rose Toppo (38110458) who carried out the project entitled Indian Sign Language Recognition \\nSystem to Help Mute and Deaf People under my supervision  from _____________ to ____________.  \\n \\n \\n \\n \\n \\nInternal Guide                       External Guide  \\n(MS. DEEPA, DR. BEVISH)         (DR. BEVISH)  \\n \\n \\n \\n \\nHead of the Department  \\n(DR.L. LAKSHMANAN and DR.S. VIGNESHWARI)  \\n \\nSubmitted for Viva -voce Examination held on ________________________________  \\n \\n \\nInternal Examiner                                                                                         External Examiner  \\n \\n \\n \\n4 \\n                                                           ACKNOWLEDGEMENT  \\n \\n \\n \\nI am pleased to acknowledge my sincere thanks to Board of Management of SATHYABAMA for \\ntheir kind encouragement in doing this project and for completing it successfully. I am grateful to \\nthem.  \\n \\n \\n \\nI convey my thanks to Dr. T. Sasikala M.E., Ph.D., Dean, Sch ool of Computer Science and \\nTechnology and Dr. L. Lakshmanan, Head of the Department, Dept. of Computer Science and \\nTechnology for providing me necessary support and details at the right time during the progressive \\nreviews.  \\n \\n \\nI would like to express my sin cere and deep sense of gratitude to my Project Guide Dr. B. Bharathi \\nfor her valuable guidance, suggestions and constant encouragement paved way for the successful \\ncompletion of my project work . \\n \\n \\nI wish to express my thanks to all Teaching and Non -teachin g staff members of the Department of \\nComputer Science and Engineering who were helpful in many ways for the completion of the \\nproject.  \\n \\n \\n \\n \\n \\n \\n \\n  5 \\n INDIAN SIGN LANGUAGE RECOGNITION SYSTEM TO HELP DEAF AND MUTE \\nPEOPLE  \\n \\nSubmitted in partial fulfillment of the requirements for the award of Bachelor of Engineering / \\nTechnology degree in Computer Science and Engineering (Specialisation of degree)  \\n \\nBy \\nPitambara Awadhesh (38110406)  \\nReema Rose Toppo (38110458)  \\n \\n \\n \\nDEPARTMENT OF  COMPUTER SCIENCE AND ENGINEERING  \\nSCHOOL OF COMPUTER SCIENCE AND ENGINEERING  \\n \\nSATHYABAMA  \\nINSTITUTE OF SCIENCE AND TECHNOLOGY  \\n(DEEMED TO BE UNIVERSITY)  \\nAccredited with Grade “A” by NAAC  \\nJEPPIAAR NAGAR, RAJIV GANDHI SALAI, CHENNAI - 600119  \\n \\nMAY - 2022 \\n  \\n6 \\n                                             ABSTRACT  \\n \\n \\n \\n Everything has changed due to the COVID19 pandemic. We went from offline to \\ninternet mode in no time. Some people found it easy to adjust to this way of life, but \\nmany others with disabilities never did and still find it difficult to explain their ideas i n \\nonline meetings. To solve this problem, we have proposed a Convolutional Neural \\nNetwork (CNN) based model for Indian sign language recognition. In our proposed \\nmethod, a mute or deaf person can interact with the camera integrated into a computer \\nand use gestures that will be recognized and converted to text for others to understand. \\nFor this, we have created a sample dataset and pre -processed it using a label binarizer. \\nAfterward, feature extraction was done using two models, first for the palm region and  \\nthe other for the fingers. Later on, this dataset was fed into a custom -made CNN model \\nwhose learning parameters were provided as 0.001 learning rate, 128 batch -size, and \\n10 epochs. The model performed well with 93% of accuracy while recognizing a hand \\ngesture. Hence, our proposed model can be integrated into other online meeting sites \\nfor the target audience to use.   7 \\n  \\nCONTENTS  \\nCHAPTERS  TITLE  PAGE  \\n Bonafide  2 \\n Declaration  3 \\n Acknowledgement  4 \\n Certificate  5 \\n Abstract  6 \\n   \\n1 INTRODUCTION   \\n   \\n1.1 Basics  8 \\n1.2 Python3  8 \\n1.3 Python Modules  9-11 \\n1.4 Deep Learning and Types  12-13 \\n1.5 OpenCV  14 \\n1.6 Tensorflow  15 \\n   \\n2 Literature Survey   \\n2.1 Civil airline fare prediction with a multi -attribute dual -\\nstage attention mechanism  16 \\n2.2 AirFare Prediction  17 \\n   \\n3 Materials and Methods used   \\n3.1 EDA 18 \\n3.2 Data -sets 18 \\n3.3 Proposed Methodology and flowchart  19 \\n3.4 Creating WebApp  20 \\n   \\n4 Results and Conclusions   \\n4.1 Categorical data Graphs  21 \\n4.2 Graphs representing feature selection  22-23 \\n4.3 Results and estimation  24 \\n   \\n5 Summary and Future work  25 \\n References  27 \\n   \\n   \\n   \\n   \\n   \\n   8 \\n CHAPTER 1 - INTRODUCTION  \\n \\n \\n1.1 Basics  \\n \\nThe goal of this project was to build a neural network able to classify which word of the Indian \\nSign Language (ISL) is being signed, given an image of a signing hand. This project is a first step \\ntowards building a possible sign language translator, which  can take communications in sign \\nlanguage and translate them into written and oral language. Such a translator would greatly lower \\nthe barrier for many deaf and mute individuals to be able to better communicate with others in \\nday-to-day interactions.  \\nThis goal is further motivated by the isolation that is felt within the deaf community. Loneliness \\nand depression exist at higher rates among the deaf population, especially when they are \\nimmersed in a hearing world. Large barriers that profoundly affect life quality stem from the \\ncommunication disconnect between the deaf and the hearing. Some examples are information \\ndeprivation, limitation of social connections, and difficulty integrating in society.  \\nMost research implementations for this task have used dept h maps generated by the depth \\ncamera and high -resolution images. The objective of this project was to see if neural networks are \\nable to classify signed ISL letters using an input video sequence of a person taken with a personal \\ndevice such as a laptop web cam. This is in alignment with the motivation as this would make a \\nfuture implementation of a real -time ISL -to-oral/written language translator practical in everyday \\nsituations.  \\n \\n1.2  Python3  \\n \\nPython is an interpreted, high, general -purpose programming language . Created by Guido \\nvan Rossum and first released in 1991, Python\\'s design philosophy emphasizes code \\nreadability with its notable use of significant whitespace . Its language constructs and \\nobject -oriented  approach aim to help programmers write clear, logical code for small and 9 \\n large -scale projects. Python is dynamically typed and garbage -collected . It supports \\nmultiple programming paradigms  including structured (particularly, procedural), object -\\noriented, and functional programming. Python is often  described as a \"batteries included\" \\nlanguage due to its comprehensive standard library.  \\n \\n \\n1.3 Analysis and Visualization of Data  \\n \\n \\n(a) NumPy  \\n \\nNumPy is a python library used for working with arrays. It also has functions for \\nworking in  the domain of linear algebra, Fourier  transform, and matrices. NumPy \\nwas created in 2005 by Travis Oliphant. It is an open -source  project and you can \\nuse it freely. NumPy stands for Numerical Python.  \\n \\n \\nWhy NumPy?  \\nIn Python , we have lists that serve the \\npurpose of arrays, but they are slow to \\nprocess. NumPy aims to provide an \\narray object that is up to 50x faster than \\ntraditional Python lists.  \\n \\nThe array object in NumPy is called \\nndarray, in  \\n \\n \\n \\n \\n \\nFigure 1: NumPy\\nprovides a lot of supporting functions that make working with ndarray very easy. Arrays \\nare very frequently used in data science, where speed and resources are very \\nimportant.  \\n \\n \\n \\n(b) Pandas  \\n \\n10 \\n Pandas is a high -level data manipulation tool developed by Wes McKinney. It is built \\non the Numpy package and its key data structure is called the DataFrame. \\nDataFrames allow you to store and manipulate tabular data in rows of observations \\nand columns of v ariables.11 \\n  \\n \\n \\n \\n \\n \\n \\nFigure 2: Pandas: DataFrames  \\n \\n \\nDataFrame object for data manipulation with integrated indexing.  \\n \\n▪ Tools for reading and writing data between in -memory data structures and different file formats.  \\n▪ Data alignment and integrated handling of missing data.  \\n▪ Reshaping and pivoting of data sets.  \\n▪ Label -based slicing, fancy indexing, and subsetting of large data sets.  \\n▪ Data structure column insertion and deletion.  \\n▪ Group by engine allowing split -apply -combine operations on data sets.  \\n▪ Data set merging and joining.  \\n▪ Hierarchical axis indexing to work with high -dimensional data in a lower -dimensional data structure.  \\n▪ Time series -functionality: Date range generation and frequency conversion, moving window \\nstatistics, moving window linear regressions, date shifting , and lagging.  \\n▪ Provides data filtration.  \\n \\n \\n \\n(c)  MatPlotlib  \\n12 \\n Matplotlib is a comprehensive library for creating static, animated, and interactive \\nvisualizations in Python. Matplotlib produces publication -quality figures in a variety of \\nhardcopy formats and interactive environments across platforms. Matplotlib can be  \\nused in Python scripts, the Python and IPython shell, web application servers, and \\nvarious graphical user interface toolkits.  \\n \\n \\n \\nFigure 3: Plots using Matplotlib  \\n \\n \\n \\n \\n• Convenient views onto the overall structure of complex data -set\\n13 \\n • High-level abstractions for structuring multi -plot grids that let you easily \\nbuild complex visualizations  \\n• Concise control over matplotlib figure styling with several built -in themes  \\n• Tools for choosing color palettes that faithfully reveal patterns in your data \\n \\n \\n1.4 Deep Learning  \\n \\nDeep learning is a subset of  machine learning, which is essentially a neural network \\nwith three or more layers. These neural networks attempt to simulate the behavior  of the \\nhuman brain —albeit far from matching its ability —allowing it to ―learn‖ from large \\namounts of data. While a neural network with a single layer can still make approximate \\npredictions, additional hidden layers can help to optimize and refine for accu racy.  \\nDeep learning drives much  artificial intelligence (AI)  applications and services that \\nimprove automation, performing analytical and physical tasks without human \\nintervention.  \\nDeep neural networks consist of multiple layers of interconnected nodes, ea ch building \\nupon the previous layer to refine and optimize the prediction or categorization. This \\nprogression of computations through the network is called forward propagation. The \\ninput and output layers of a deep neural network are called  visible  layers.  The input \\nlayer is where the deep learning model ingests the data for processing, and the output \\nlayer is where the final prediction or classification is made.  \\nTo state the layer in a clear manner:  \\n1. Input layer – The input layer has input features in a dat aset that is known to us.  \\n2. Hidden Layer – Hidden layer, just like we need to train the brain through hidden \\nneurons.  \\n3. Output layer – value that we want to classify.  14 \\n  \\n \\nFig 5: Types of Deep learning algorithms  \\n \\n \\nThere are many types of deep learning algorithms developed over the years but there \\nare a few algorithms that are frequently used:  \\n1. Artificial Neural Network:  \\n \\nAn artificial Neural Network is the component of a computing system designed in such a \\nway that the human brain analyses and makes a decision. Ann is the building block of \\ndeep learning and solves problems  that seem  impossible or very difficult to humans.  \\n15 \\n Artificial neural networks work like a human brain. The human brain has billions of \\nneurons and each neuron is made up of a cell body that is responsible for computing \\ninformation by carrying forward information towards hidden neurons and providing  the \\nfinal Output.  \\n                         \\n    Fig 6: Artificial Neural Networks  \\n \\nThe aim is to minimize the error by adjusting the weight and bias of the interconnection \\nwhich is known as backpropagation. With the process of backpropagation, the \\ndifference between the desired output and actual output produces the least error.  \\n2. Convoluti onal Neural Network  \\n \\nCNN is a supervised type of Deep learning, most preferable used in image recognition \\nand computer vision. CNN has multiple layers that process and extract important \\nfeatures from the image.  Convolutional neural networks are composed of multiple layers \\nof artificial neurons. Artificial neurons, a rough imitation of their biological counterparts, \\nare mathematical functions that calculate the weighted sum of multiple inputs and \\noutput  an activat ion value. When you input an image in a ConvNet, each layer \\ngenerates several activation functions that are passed on to the next layer.  \\n16 \\n The first layer usually extracts basic features such as horizontal or diagonal edges. This \\noutput is passed on to the n ext layer which detects more complex features such as \\ncorners or combinational edges. As we move deeper into the network it can identify \\neven more complex features such as objects, faces, etc . \\n \\nFig 7: Working of a CNN  \\n \\n3. Recurrent Neural Networks (RNNs)  \\n \\nRNN is a type of supervised deep learning where the output from the previous step is \\nfed as input to the current step. RNN deep learning algorithm is best suited for \\nsequential data. RNN is most preferably used in image captioning, time -series analysis, \\nnatural -language processing, handwriting recognition, and machine translation.  \\n17 \\n  \\n \\n \\nFig 8: Recurrent Neural Networks (RNNs)  \\n \\nThe most vital feature of RNN is the Hidden state, which memorizes some information \\nabout a sequence. There are mainly 4 steps of how RNN works.  \\n1. The output of the hidden state at t -1 is fed into input at time t.  \\n2. Same way, the output at time t fed into the input at time t+1.  \\n3. RNN can process inputs of any considerable length  \\n4. The RNN computation depends on historical sequence data and the model size \\ndoesn’t increase with input size . \\n \\n1.5 OpenCV  \\n \\n \\nOpenCV is a great tool for image processing and performing computer vision tasks. It is \\nan open -source library that can be used to perform tasks like face detection, objection \\n18 \\n tracking, landmark detection, and much more. It supports multiple languages incl uding \\npython, java C++. Although, For this article, we will be limiting to python only.  \\nThe library is equipped with hundreds of useful functions and algorithms, which are all \\nfreely available to us. Some of these functions are really common and are used in \\nalmost every computer vision task. Whereas many of the functions are still unexplored \\nand haven’t received much attention yet.  \\nWith the help of Open CV in python, it\\'s possible to process images, videos easily and \\ncan extract useful information from the m, as there are lots of functions available. Some \\nof the common applications are  \\n  \\n1. Image Processing:  \\nImages can be read, written, shown, and processed with the OpenCV, which can \\ngenerate a new image from that either by changing its shape, color, or extrac ting \\nsomething useful from the given one and write into a new image.  \\n  \\n2. Face Detection:  \\nEither from the live streaming using a web camera or from the locally stored \\nvideos/images utilizing Haar -Cascade Classifiers.  \\n \\n3. Face Recognition:  \\nIt is followed by face detection from the videos using OpenCV by drawing \\nbounding boxes i.e. rectangles and then model training using ML algorithms to \\nrecognize faces.  \\n \\n \\n \\n4. Object Detection:  19 \\n Open CV along with the YOLO, an object detection algorithm can be used to \\ndetect objects from the image, videos either moving or stationary objects.  \\n \\n    \\n  \\n     \\n \\n1.6 TensorFlow  \\n \\n \\nFig 9: TensorFlow  \\n \\nTensorFlow is an open -source library developed by Google primarily for deep learning \\napplications. It also supports traditional machine learning. TensorFlow was originally \\ndeveloped for large numerical computations without keeping deep learning in mind. \\nHowever, it proved to be very useful for deep learning development as well, and \\ntherefore Google open -sourced it.  \\nTensorFlow accepts data in the form of multi -dimensional arrays of higher dimensions \\ncalled tensors. Multi -dimensional arrays are very handy in handling large amounts of \\ndata.  \\nTensorFlow works on the basis of data flow graphs that have nodes and edges. As the \\nexecution mechanism is in the form of graphs, it is much easier to execute TensorFlow \\ncode in a distributed manner across a cluster of compu ters while using GPUs.  \\n20 \\n  \\n1.6.1 Tensorflow Object Detection API  \\nCreating accurate machine learning models capable of localizing and identifying \\nmultiple objects in a single image remains a core challenge in computer vision. The \\nTensorFlow Object Detection AP I is an open -source framework built on top of \\nTensorFlow that makes it easy to construct, train and deploy object detection models.  \\n \\nFig 10: Object Detection in Tensorflow  \\n \\n \\n \\n  \\n21 \\n CHAPTER 2 – LITERATURE SURVEY  \\n \\n \\n2.1 Vision -based Real -Time Hand Gesture Recognition Techniques for Human -\\nComputer Interaction (IEEE)  \\n \\nAt first, we started studying the most basic human -computer interaction and how it \\nactually works. Ghotkar and Kharate explained three techniques and experime nted \\nwith interaction with a Desktop/Laptop with static hand gestures. All these techniques \\nused a real -time approach with different feature descriptors such as Fourier \\nDescriptor(FD),7 Hu moments, Convex Hull, and Finger Detection. Real -time \\nRecognition e fficiency was calculated with respect to recognition time for FD and 7 Hu \\nmoments.  \\n \\nThere are two major approaches for hand gesture recognition: Data Glove, Vision -\\nbased. Each approach is having its limitations and advantages, but vision -based \\napproaches are more feasible as compared to data gloves as users do not need to \\nwear cumbersome devices like data gloves.  \\n \\nVision -based hand gesture recognition is having challenges such as variable lighting \\nconditions, dynamic background, and skin color detection, considering these fact \\nalgorithms were developed to overcome some of these challenges.  Recognition of \\nhand gestures executes windows applications such as opening Notepad, Windows \\nmedia player, Internet Explorer, and MS -Paint. All algorithms were working on bare \\nhands where the user need not wear any color gloves or data gloves . \\n \\n 22 \\n   23 \\n  \\n2.2 Real-Time Hand Gesture Recognition Using Finger Segmentation (Hindawi)  \\n \\n \\nIn this paper, Zhi -Hua Chen and Jung -Tae Kim present an efficient and effective method \\nfor hand gesture recognition. The hand region is detected through the background \\nsubtraction method. Then, the palm and fingers are split so as to recognize the fingers. \\nAfter the fingers are recognized, the hand gesture can be classified through a simple \\nrule classifier.  \\n \\nFig 11: Base paper methodology  \\n \\n \\n2.3 Tracking of dynamic gesture fingertips position in a video sequence  \\n \\n \\nThe field of research of this paper combines Human -Computer Interface, gesture \\nrecognition, and fingertip tracking. Most gesture recognition algorithms processing color \\nimages are unable to locate folded fingers hidden inside hand contour. With the use of \\nhand landmarks detection and localization alg orithm, processing directional images, the \\nfingertips are tracked whether they are risen or folded inside the hand contour. The \\ncapabilities of the method, repeatability, and accuracy, are tested with the use of 3 \\ngestures that are recorded on the USB came ra. Fingertips are tracked in gestures \\npresenting a linear movement of an open hand, finger folding into a fist, and clenched \\nfist movement. In conclusion, a discussion of accuracy in application to HCI is \\npresented.  \\n \\n24 \\n  \\n \\n2.4  \\n \\n  25 \\n CHAPTER 4  -EXPERIMENTAL OR MATERIALS AND METHODS; ALGORITHMS USED  \\n \\n \\n \\n3.1 Exploratory Data Analysis  \\n \\nConfusion Matrix  \\n \\n \\n3.2 Dataset  \\n \\nThe dataset is prepared by enabling a webcam.  Labels should be binarized in a one -\\nto-all fashion.Scikit -learn includes a number of regression and binary classification \\ntechniques. The so -called one -vs-all approach is a straightforward way to extend these \\nalgorithms to the multi -class classification p roblem.This basically entails learning one \\nregressor or binary classifier per class throughout learning time. To do so, multi -class \\nlabels must be converted to binary labels (belong or do not belong to the class). The \\ntransform method in LabelBinarizer mak es this operation simple.When it comes to \\nprediction, one assigns the class for which the matching model provided the highest \\nlevel of confidence. The inverse transform method in LabelBinarizer makes this \\nsimple.The dataset was mostly built utilising a liv e video feed to capture all of the signs \\nfrom A to Z and 1 to 9 in jpeg format.Some experimental data has been acquired by \\ninfluencing and changing the data.It accomplishes the study\\'s goal of recognising \\nIndian sign language from a video feed.  \\n \\n3.3 Proposed Methodology  \\n \\nCNN (Convolutional Neural Networks) is used in the suggested system. It\\'s a four -layer \\nmodified CNN model that starts with three nodes.Modules like the torch were used to \\nload, divide, and test the data during the training phase.The e pochs were set to ten, and \\nthe device was examined to determine whether Cuda was available; if it wasn\\'t, the \\nCPU was used.The photo positions and labels were afterwards obtained from the 26 \\n data.csv file. Adam and Cross -Entropy Loss are the optimizer and cri terion used for the \\nfunctions, respectively.Each predicted class probability is compared to the real class \\nintended result, which is either 0 or 1, to determine a score/loss that penalises the \\nprobability based on how far it deviates from the actual expect ed value.Data loader, \\noptimizer, criterion, model, and train data are used to produce a function called \"fit\" for \\ntraining.The parameters data loader, optimizer, criterion, model, and valid data are \\nsupplied to another validation function called \"validate. \"The validate loss and accuracy \\nare calculated using this function.Learning rate = 0.001, batch size = 128, and epochs = \\n10 are specified as learning parameters. Within the period range, \"fit\" is called to \\nprovide accuracy and loss for training.The model i s retained for future use after it has \\nbeen trained.To open the web camera and then show the gesture, which is anticipated, \\nand the word is displayed on the screen to receive the output and truly predict a hand \\nsign.  \\n \\n \\nFigure 6: Inception V3 Architecture  \\n \\n \\n \\n \\n \\n \\n \\n27 \\n 3.3 Flowchart  \\n \\n \\nFigure 7 : Flowchart  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n28 \\n CHAPTER 5 – RESULTS AND DISCUSSIONS;PERFORMANCE ANALYSIS  \\n \\n \\n \\n5.1 Training and Validating  \\n \\nThe data is trained using the CPU. As a result, CUDA is identified as zero (without \\nGPU). The 42000 dataset is split into two parts: one for training and the other for \\nvalidation.Training set: is a collection of samples used to learn how to fit the classif ier\\'s \\nparameters [i.e., weights]. Validation set: A set of examples used to fine -tune a \\nclassifier\\'s parameters [architecture, not weights], such as the number of hidden units in \\na neural network.  \\n \\n \\nFigure 8 : Number of files  \\nThe CustomCNN function create d is displayed below, which will be combined with the \\nInception Model to train the dataset.  \\n \\n \\nFigure 8: Custom CNN Architecture  \\n29 \\n  \\nThe model is then fitted with two functions: one for training and the other for validating. This will \\nproduce the results, which will be compared to the target.  \\n \\n \\n5.2 Results              \\n \\nGraph for train loss which shows the deviation of error loss while t raining. A loss \\nfunction, also known as a cost function, is a function that transfers an event or the \\nvalues of one or more variables onto a real number that intuitively represents some \\n\"cost\" connected with the event in mathematical optimization and decis ion theory. This \\nis the initial data epochs vs error loss. An epoch is a word used in machine learning that \\nrefers to the number of passes the machine learning algorithm has made across the full \\ntraining dataset. At 0 epoch, loss calculated is approx. 0.00 5. \\n \\nFigure 9. Train Loss  \\n \\n30 \\n The graph below depicts the initial training accuracy, which indicates that the model was \\nsaved after a checkpoint. At 0 epoch, accuracy attained is approx 83%.As the training \\nmoves forward it changes to 98%.  \\n \\nFigure 10. Training Accuracy  \\n \\n \\n \\n31 \\n  \\n       \\n \\n \\n \\n32 \\n  \\n \\n \\n: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n33 \\n CHAPTER 6 – CONCLUSION AND FUTURE WORK  \\n \\n \\n6.1 Conclusion  \\n \\nUsing a CustomCNN classifier, the output is a fully trained model with a 93 percent \\naccuracy. With a total of 1200 samples, the visuals used were in Indian Sign Language. \\nThese photographs are organised into folders based on their meaning. The dataset was \\ndownloaded in jpeg format, then label binarized and pre -processed. We achieved a loss \\nrate of roughly 0.10 after training the model. After that, the trained model was saved and \\nreloaded. A script was used to enable the web camera. Finally, a hand gesture w as \\ndemonstrated, which was effectively detected.  \\n \\n \\n6.2 Future Work  \\n \\nIn the future, this might be modified into an application where a person practicing sign \\nlanguage can transmit live video to the other end, which can be understood by the \\nperson watching. Basically, when the video is sequenced, it will capture the main \\ngesture which classifies as the sign language, and then the recognized sign is \\nclassified. The database can be modified by adding more images to it. This will lead to \\nmaking the system more accurate in detecting everything.  \\n \\n \\n \\n \\n \\n  34 \\n REFERENCES  \\n \\n \\nNumPy, Pandas, MatplotLib: GeekForGeeks: geekforgeeks.org  \\n \\nPython3: www.python.org/download/releases/3.0/  \\n \\nMachine Learning : towardsdatascience.com  \\n \\n \\n[1] Study of vision -based hand gesture recognition using Indian sign language in \\nINTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS \\nVOL. 7, NO. 1, MARCH 2014.  \\n[2] Real-time hand gesture recognition using finger segmentation in THE SCIENTIFIC \\nWORLD JOURNAL (JAN 2014).  \\n[3] Hand sign recognition from depth images with multi -scale density features for deaf -\\nmute persons in International Conference on Computational Intelligence and Data \\nScience (ICCIDS 2019).  \\n \\n[4] Indian sign language interpreter using image processing and machine learning in \\nIOP Conference Series: Materials Science and Engineering 2020.  \\n \\n[5] Dynamic Hand Gesture Recognition: A Literature Review in IJERT 2012.  \\n \\n[6] Hand Gesture Recognition A Literature Review in IJSRD - International Journal for \\nScientific Research & Development| Vol. 8, Issue 2, 2020.  \\n[7] Tracking of dynamic gesture fingertips position in a video sequence in Archives of \\nControl Sciences Volume 30(LX VI), 2020.  \\n  [8] Visual tracking utilizing robust complementary learner and adaptive refiner in \\nScience   Direct,10th May 2017  35 \\n [9] Tracking of dynamic gesture fingertips position in a video sequence in Archives of \\nControl Sciences Volume 30(LXVI), 2020.  \\n[10] Visual tracking utilizing robust complementary learner and adaptive refiner in \\nScience Direct,10th May 2017.  \\n[11] Sign Language Recognition Using Image Processing, Research Gate \\n2017.Development of the Recognition System of Japanese Sign Language Using  3D \\nImage Sensor (HCI International 2013).  \\n \\n \\n \\n \\n '"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1 \\n  \\nFINAL YEAR PROJECT REPORT  \\nat \\nSathyabama Institute of Science and Technology (Deemed to be \\nUniversity)  \\n \\nSubmitted in partial fulfilment of the requirements for the award of Bachelor of \\nEngineering Degree in Computer Science and Engineering  \\n \\n \\n \\n                                                              Pitambara Awadhesh  \\n(Reg. No. 38110406)  \\nReema Rose Toppo  \\n(Reg. No. 38110458)  \\n \\n \\n \\n \\nDEPARTMENT OF COMPUTER SCIENCE AND ENGINEERING SCHOOL OF \\nCOMPUTING  \\nSATHYABAMA INSTITUTE OF SCIENCE AND TECHNOLOGY JEPPIAAR NAGAR, \\nRAJIV GANDHI SALAI,  \\nCHENNAI – 600119, TAMILNADU  \\n2 \\n 3 \\n                                    SATHYABAMA  \\n                                                           INSTITUTE OF SCIENCE AND TECHNOLOGY  \\n(DEEMED TO BE UNIVERSITY)  \\n                                                 Accredited with Grade “A” by NAAC  \\n                                                        (Established under Section 3 of UGC Act, 1956)  \\nJEPPIAAR NAGAR, RAJIV GANDHI SALAI, CHENNAI – 600119  \\nwww.sathyabamauniversity.ac.in  \\n \\n_________________________________________________________________________________  \\nDEPARTMENT OF COMPUTER SCIENCE AND ENGINEERING  \\n \\nBONAFIDE CERTIFICATE  \\n \\nThis is to certify that this Project Report is the bonafide work of Pitambara Awadhesh (38110406) and \\nReema Rose Toppo (38110458) who carried out the project entitled Indian Sign Language Recognition \\nSystem to Help Mute and Deaf People under my supervision  from _____________ to ____________.  \\n \\n \\n \\n \\n \\nInternal Guide                       External Guide  \\n(MS. DEEPA, DR. BEVISH)         (DR. BEVISH)  \\n \\n \\n \\n \\nHead of the Department  \\n(DR.L. LAKSHMANAN and DR.S. VIGNESHWARI)  \\n \\nSubmitted for Viva -voce Examination held on ________________________________  \\n \\n \\nInternal Examiner                                                                                         External Examiner  \\n \\n \\n \\n4 \\n                                                           ACKNOWLEDGEMENT  \\n \\n \\n \\nI am pleased to acknowledge my sincere thanks to Board of Management of SATHYABAMA for \\ntheir kind encouragement in doing this project and for completing it successfully. I am grateful to \\nthem.  \\n \\n \\n \\nI convey my thanks to Dr. T. Sasikala M.E., Ph.D., Dean, Sch ool of Computer Science and \\nTechnology and Dr. L. Lakshmanan, Head of the Department, Dept. of Computer Science and \\nTechnology for providing me necessary support and details at the right time during the progressive \\nreviews.  \\n \\n \\nI would like to express my sin cere and deep sense of gratitude to my Project Guide Dr. B. Bharathi \\nfor her valuable guidance, suggestions and constant encouragement paved way for the successful \\ncompletion of my project work . \\n \\n \\nI wish to express my thanks to all Teaching and Non -teachin g staff members of the Department of \\nComputer Science and Engineering who were helpful in many ways for the completion of the \\nproject.  \\n \\n \\n \\n \\n \\n \\n \\n  5 \\n INDIAN SIGN LANGUAGE RECOGNITION SYSTEM TO HELP DEAF AND MUTE \\nPEOPLE  \\n \\nSubmitted in partial fulfillment of the requirements for the award of Bachelor of Engineering / \\nTechnology degree in Computer Science and Engineering (Specialisation of degree)  \\n \\nBy \\nPitambara Awadhesh (38110406)  \\nReema Rose Toppo (38110458)  \\n \\n \\n \\nDEPARTMENT OF  COMPUTER SCIENCE AND ENGINEERING  \\nSCHOOL OF COMPUTER SCIENCE AND ENGINEERING  \\n \\nSATHYABAMA  \\nINSTITUTE OF SCIENCE AND TECHNOLOGY  \\n(DEEMED TO BE UNIVERSITY)  \\nAccredited with Grade “A” by NAAC  \\nJEPPIAAR NAGAR, RAJIV GANDHI SALAI, CHENNAI - 600119  \\n \\nMAY - 2022 \\n  \\n6 \\n                                             ABSTRACT  \\n \\n \\n \\n Everything has changed due to the COVID19 pandemic. We went from offline to \\ninternet mode in no time. Some people found it easy to adjust to this way of life, but \\nmany others with disabilities never did and still find it difficult to explain their ideas i n \\nonline meetings. To solve this problem, we have proposed a Convolutional Neural \\nNetwork (CNN) based model for Indian sign language recognition. In our proposed \\nmethod, a mute or deaf person can interact with the camera integrated into a computer \\nand use gestures that will be recognized and converted to text for others to understand. \\nFor this, we have created a sample dataset and pre -processed it using a label binarizer. \\nAfterward, feature extraction was done using two models, first for the palm region and  \\nthe other for the fingers. Later on, this dataset was fed into a custom -made CNN model \\nwhose learning parameters were provided as 0.001 learning rate, 128 batch -size, and \\n10 epochs. The model performed well with 93% of accuracy while recognizing a hand \\ngesture. Hence, our proposed model can be integrated into other online meeting sites \\nfor the target audience to use.   7 \\n  \\nCONTENTS  \\nCHAPTERS  TITLE  PAGE  \\n Bonafide  2 \\n Declaration  3 \\n Acknowledgement  4 \\n Certificate  5 \\n Abstract  6 \\n   \\n1 INTRODUCTION   \\n   \\n1.1 Basics  8 \\n1.2 Python3  8 \\n1.3 Python Modules  9-11 \\n1.4 Deep Learning and Types  12-13 \\n1.5 OpenCV  14 \\n1.6 Tensorflow  15 \\n   \\n2 Literature Survey   \\n2.1 Civil airline fare prediction with a multi -attribute dual -\\nstage attention mechanism  16 \\n2.2 AirFare Prediction  17 \\n   \\n3 Materials and Methods used   \\n3.1 EDA 18 \\n3.2 Data -sets 18 \\n3.3 Proposed Methodology and flowchart  19 \\n3.4 Creating WebApp  20 \\n   \\n4 Results and Conclusions   \\n4.1 Categorical data Graphs  21 \\n4.2 Graphs representing feature selection  22-23 \\n4.3 Results and estimation  24 \\n   \\n5 Summary and Future work  25 \\n References  27 \\n   \\n   \\n   \\n   \\n   \\n   8 \\n CHAPTER 1 - INTRODUCTION  \\n \\n \\n1.1 Basics  \\n \\nThe goal of this project was to build a neural network able to classify which word of the Indian \\nSign Language (ISL) is being signed, given an image of a signing hand. This project is a first step \\ntowards building a possible sign language translator, which  can take communications in sign \\nlanguage and translate them into written and oral language. Such a translator would greatly lower \\nthe barrier for many deaf and mute individuals to be able to better communicate with others in \\nday-to-day interactions.  \\nThis goal is further motivated by the isolation that is felt within the deaf community. Loneliness \\nand depression exist at higher rates among the deaf population, especially when they are \\nimmersed in a hearing world. Large barriers that profoundly affect life quality stem from the \\ncommunication disconnect between the deaf and the hearing. Some examples are information \\ndeprivation, limitation of social connections, and difficulty integrating in society.  \\nMost research implementations for this task have used dept h maps generated by the depth \\ncamera and high -resolution images. The objective of this project was to see if neural networks are \\nable to classify signed ISL letters using an input video sequence of a person taken with a personal \\ndevice such as a laptop web cam. This is in alignment with the motivation as this would make a \\nfuture implementation of a real -time ISL -to-oral/written language translator practical in everyday \\nsituations.  \\n \\n1.2  Python3  \\n \\nPython is an interpreted, high, general -purpose programming language . Created by Guido \\nvan Rossum and first released in 1991, Python\\'s design philosophy emphasizes code \\nreadability with its notable use of significant whitespace . Its language constructs and \\nobject -oriented  approach aim to help programmers write clear, logical code for small and 9 \\n large -scale projects. Python is dynamically typed and garbage -collected . It supports \\nmultiple programming paradigms  including structured (particularly, procedural), object -\\noriented, and functional programming. Python is often  described as a \"batteries included\" \\nlanguage due to its comprehensive standard library.  \\n \\n \\n1.3 Analysis and Visualization of Data  \\n \\n \\n(a) NumPy  \\n \\nNumPy is a python library used for working with arrays. It also has functions for \\nworking in  the domain of linear algebra, Fourier  transform, and matrices. NumPy \\nwas created in 2005 by Travis Oliphant. It is an open -source  project and you can \\nuse it freely. NumPy stands for Numerical Python.  \\n \\n \\nWhy NumPy?  \\nIn Python , we have lists that serve the \\npurpose of arrays, but they are slow to \\nprocess. NumPy aims to provide an \\narray object that is up to 50x faster than \\ntraditional Python lists.  \\n \\nThe array object in NumPy is called \\nndarray, in  \\n \\n \\n \\n \\n \\nFigure 1: NumPy\\nprovides a lot of supporting functions that make working with ndarray very easy. Arrays \\nare very frequently used in data science, where speed and resources are very \\nimportant.  \\n \\n \\n \\n(b) Pandas  \\n \\n10 \\n Pandas is a high -level data manipulation tool developed by Wes McKinney. It is built \\non the Numpy package and its key data structure is called the DataFrame. \\nDataFrames allow you to store and manipulate tabular data in rows of observations \\nand columns of v ariables.11 \\n  \\n \\n \\n \\n \\n \\n \\nFigure 2: Pandas: DataFrames  \\n \\n \\nDataFrame object for data manipulation with integrated indexing.  \\n \\n▪ Tools for reading and writing data between in -memory data structures and different file formats.  \\n▪ Data alignment and integrated handling of missing data.  \\n▪ Reshaping and pivoting of data sets.  \\n▪ Label -based slicing, fancy indexing, and subsetting of large data sets.  \\n▪ Data structure column insertion and deletion.  \\n▪ Group by engine allowing split -apply -combine operations on data sets.  \\n▪ Data set merging and joining.  \\n▪ Hierarchical axis indexing to work with high -dimensional data in a lower -dimensional data structure.  \\n▪ Time series -functionality: Date range generation and frequency conversion, moving window \\nstatistics, moving window linear regressions, date shifting , and lagging.  \\n▪ Provides data filtration.  \\n \\n \\n \\n(c)  MatPlotlib  \\n12 \\n Matplotlib is a comprehensive library for creating static, animated, and interactive \\nvisualizations in Python. Matplotlib produces publication -quality figures in a variety of \\nhardcopy formats and interactive environments across platforms. Matplotlib can be  \\nused in Python scripts, the Python and IPython shell, web application servers, and \\nvarious graphical user interface toolkits.  \\n \\n \\n \\nFigure 3: Plots using Matplotlib  \\n \\n \\n \\n \\n• Convenient views onto the overall structure of complex data -set\\n13 \\n • High-level abstractions for structuring multi -plot grids that let you easily \\nbuild complex visualizations  \\n• Concise control over matplotlib figure styling with several built -in themes  \\n• Tools for choosing color palettes that faithfully reveal patterns in your data \\n \\n \\n1.4 Deep Learning  \\n \\nDeep learning is a subset of  machine learning, which is essentially a neural network \\nwith three or more layers. These neural networks attempt to simulate the behavior  of the \\nhuman brain —albeit far from matching its ability —allowing it to ―learn‖ from large \\namounts of data. While a neural network with a single layer can still make approximate \\npredictions, additional hidden layers can help to optimize and refine for accu racy.  \\nDeep learning drives much  artificial intelligence (AI)  applications and services that \\nimprove automation, performing analytical and physical tasks without human \\nintervention.  \\nDeep neural networks consist of multiple layers of interconnected nodes, ea ch building \\nupon the previous layer to refine and optimize the prediction or categorization. This \\nprogression of computations through the network is called forward propagation. The \\ninput and output layers of a deep neural network are called  visible  layers.  The input \\nlayer is where the deep learning model ingests the data for processing, and the output \\nlayer is where the final prediction or classification is made.  \\nTo state the layer in a clear manner:  \\n1. Input layer – The input layer has input features in a dat aset that is known to us.  \\n2. Hidden Layer – Hidden layer, just like we need to train the brain through hidden \\nneurons.  \\n3. Output layer – value that we want to classify.  14 \\n  \\n \\nFig 5: Types of Deep learning algorithms  \\n \\n \\nThere are many types of deep learning algorithms developed over the years but there \\nare a few algorithms that are frequently used:  \\n1. Artificial Neural Network:  \\n \\nAn artificial Neural Network is the component of a computing system designed in such a \\nway that the human brain analyses and makes a decision. Ann is the building block of \\ndeep learning and solves problems  that seem  impossible or very difficult to humans.  \\n15 \\n Artificial neural networks work like a human brain. The human brain has billions of \\nneurons and each neuron is made up of a cell body that is responsible for computing \\ninformation by carrying forward information towards hidden neurons and providing  the \\nfinal Output.  \\n                         \\n    Fig 6: Artificial Neural Networks  \\n \\nThe aim is to minimize the error by adjusting the weight and bias of the interconnection \\nwhich is known as backpropagation. With the process of backpropagation, the \\ndifference between the desired output and actual output produces the least error.  \\n2. Convoluti onal Neural Network  \\n \\nCNN is a supervised type of Deep learning, most preferable used in image recognition \\nand computer vision. CNN has multiple layers that process and extract important \\nfeatures from the image.  Convolutional neural networks are composed of multiple layers \\nof artificial neurons. Artificial neurons, a rough imitation of their biological counterparts, \\nare mathematical functions that calculate the weighted sum of multiple inputs and \\noutput  an activat ion value. When you input an image in a ConvNet, each layer \\ngenerates several activation functions that are passed on to the next layer.  \\n16 \\n The first layer usually extracts basic features such as horizontal or diagonal edges. This \\noutput is passed on to the n ext layer which detects more complex features such as \\ncorners or combinational edges. As we move deeper into the network it can identify \\neven more complex features such as objects, faces, etc . \\n \\nFig 7: Working of a CNN  \\n \\n3. Recurrent Neural Networks (RNNs)  \\n \\nRNN is a type of supervised deep learning where the output from the previous step is \\nfed as input to the current step. RNN deep learning algorithm is best suited for \\nsequential data. RNN is most preferably used in image captioning, time -series analysis, \\nnatural -language processing, handwriting recognition, and machine translation.  \\n17 \\n  \\n \\n \\nFig 8: Recurrent Neural Networks (RNNs)  \\n \\nThe most vital feature of RNN is the Hidden state, which memorizes some information \\nabout a sequence. There are mainly 4 steps of how RNN works.  \\n1. The output of the hidden state at t -1 is fed into input at time t.  \\n2. Same way, the output at time t fed into the input at time t+1.  \\n3. RNN can process inputs of any considerable length  \\n4. The RNN computation depends on historical sequence data and the model size \\ndoesn’t increase with input size . \\n \\n1.5 OpenCV  \\n \\n \\nOpenCV is a great tool for image processing and performing computer vision tasks. It is \\nan open -source library that can be used to perform tasks like face detection, objection \\n18 \\n tracking, landmark detection, and much more. It supports multiple languages incl uding \\npython, java C++. Although, For this article, we will be limiting to python only.  \\nThe library is equipped with hundreds of useful functions and algorithms, which are all \\nfreely available to us. Some of these functions are really common and are used in \\nalmost every computer vision task. Whereas many of the functions are still unexplored \\nand haven’t received much attention yet.  \\nWith the help of Open CV in python, it\\'s possible to process images, videos easily and \\ncan extract useful information from the m, as there are lots of functions available. Some \\nof the common applications are  \\n  \\n1. Image Processing:  \\nImages can be read, written, shown, and processed with the OpenCV, which can \\ngenerate a new image from that either by changing its shape, color, or extrac ting \\nsomething useful from the given one and write into a new image.  \\n  \\n2. Face Detection:  \\nEither from the live streaming using a web camera or from the locally stored \\nvideos/images utilizing Haar -Cascade Classifiers.  \\n \\n3. Face Recognition:  \\nIt is followed by face detection from the videos using OpenCV by drawing \\nbounding boxes i.e. rectangles and then model training using ML algorithms to \\nrecognize faces.  \\n \\n \\n \\n4. Object Detection:  19 \\n Open CV along with the YOLO, an object detection algorithm can be used to \\ndetect objects from the image, videos either moving or stationary objects.  \\n \\n    \\n  \\n     \\n \\n1.6 TensorFlow  \\n \\n \\nFig 9: TensorFlow  \\n \\nTensorFlow is an open -source library developed by Google primarily for deep learning \\napplications. It also supports traditional machine learning. TensorFlow was originally \\ndeveloped for large numerical computations without keeping deep learning in mind. \\nHowever, it proved to be very useful for deep learning development as well, and \\ntherefore Google open -sourced it.  \\nTensorFlow accepts data in the form of multi -dimensional arrays of higher dimensions \\ncalled tensors. Multi -dimensional arrays are very handy in handling large amounts of \\ndata.  \\nTensorFlow works on the basis of data flow graphs that have nodes and edges. As the \\nexecution mechanism is in the form of graphs, it is much easier to execute TensorFlow \\ncode in a distributed manner across a cluster of compu ters while using GPUs.  \\n20 \\n  \\n1.6.1 Tensorflow Object Detection API  \\nCreating accurate machine learning models capable of localizing and identifying \\nmultiple objects in a single image remains a core challenge in computer vision. The \\nTensorFlow Object Detection AP I is an open -source framework built on top of \\nTensorFlow that makes it easy to construct, train and deploy object detection models.  \\n \\nFig 10: Object Detection in Tensorflow  \\n \\n \\n \\n  \\n21 \\n CHAPTER 2 – LITERATURE SURVEY  \\n \\n \\n2.1 Vision -based Real -Time Hand Gesture Recognition Techniques for Human -\\nComputer Interaction (IEEE)  \\n \\nAt first, we started studying the most basic human -computer interaction and how it \\nactually works. Ghotkar and Kharate explained three techniques and experime nted \\nwith interaction with a Desktop/Laptop with static hand gestures. All these techniques \\nused a real -time approach with different feature descriptors such as Fourier \\nDescriptor(FD),7 Hu moments, Convex Hull, and Finger Detection. Real -time \\nRecognition e fficiency was calculated with respect to recognition time for FD and 7 Hu \\nmoments.  \\n \\nThere are two major approaches for hand gesture recognition: Data Glove, Vision -\\nbased. Each approach is having its limitations and advantages, but vision -based \\napproaches are more feasible as compared to data gloves as users do not need to \\nwear cumbersome devices like data gloves.  \\n \\nVision -based hand gesture recognition is having challenges such as variable lighting \\nconditions, dynamic background, and skin color detection, considering these fact \\nalgorithms were developed to overcome some of these challenges.  Recognition of \\nhand gestures executes windows applications such as opening Notepad, Windows \\nmedia player, Internet Explorer, and MS -Paint. All algorithms were working on bare \\nhands where the user need not wear any color gloves or data gloves . \\n \\n 22 \\n   23 \\n  \\n2.2 Real-Time Hand Gesture Recognition Using Finger Segmentation (Hindawi)  \\n \\n \\nIn this paper, Zhi -Hua Chen and Jung -Tae Kim present an efficient and effective method \\nfor hand gesture recognition. The hand region is detected through the background \\nsubtraction method. Then, the palm and fingers are split so as to recognize the fingers. \\nAfter the fingers are recognized, the hand gesture can be classified through a simple \\nrule classifier.  \\n \\nFig 11: Base paper methodology  \\n \\n \\n2.3 Tracking of dynamic gesture fingertips position in a video sequence  \\n \\n \\nThe field of research of this paper combines Human -Computer Interface, gesture \\nrecognition, and fingertip tracking. Most gesture recognition algorithms processing color \\nimages are unable to locate folded fingers hidden inside hand contour. With the use of \\nhand landmarks detection and localization alg orithm, processing directional images, the \\nfingertips are tracked whether they are risen or folded inside the hand contour. The \\ncapabilities of the method, repeatability, and accuracy, are tested with the use of 3 \\ngestures that are recorded on the USB came ra. Fingertips are tracked in gestures \\npresenting a linear movement of an open hand, finger folding into a fist, and clenched \\nfist movement. In conclusion, a discussion of accuracy in application to HCI is \\npresented.  \\n \\n24 \\n  \\n \\n2.4  \\n \\n  25 \\n CHAPTER 4  -EXPERIMENTAL OR MATERIALS AND METHODS; ALGORITHMS USED  \\n \\n \\n \\n3.1 Exploratory Data Analysis  \\n \\nConfusion Matrix  \\n \\n \\n3.2 Dataset  \\n \\nThe dataset is prepared by enabling a webcam.  Labels should be binarized in a one -\\nto-all fashion.Scikit -learn includes a number of regression and binary classification \\ntechniques. The so -called one -vs-all approach is a straightforward way to extend these \\nalgorithms to the multi -class classification p roblem.This basically entails learning one \\nregressor or binary classifier per class throughout learning time. To do so, multi -class \\nlabels must be converted to binary labels (belong or do not belong to the class). The \\ntransform method in LabelBinarizer mak es this operation simple.When it comes to \\nprediction, one assigns the class for which the matching model provided the highest \\nlevel of confidence. The inverse transform method in LabelBinarizer makes this \\nsimple.The dataset was mostly built utilising a liv e video feed to capture all of the signs \\nfrom A to Z and 1 to 9 in jpeg format.Some experimental data has been acquired by \\ninfluencing and changing the data.It accomplishes the study\\'s goal of recognising \\nIndian sign language from a video feed.  \\n \\n3.3 Proposed Methodology  \\n \\nCNN (Convolutional Neural Networks) is used in the suggested system. It\\'s a four -layer \\nmodified CNN model that starts with three nodes.Modules like the torch were used to \\nload, divide, and test the data during the training phase.The e pochs were set to ten, and \\nthe device was examined to determine whether Cuda was available; if it wasn\\'t, the \\nCPU was used.The photo positions and labels were afterwards obtained from the 26 \\n data.csv file. Adam and Cross -Entropy Loss are the optimizer and cri terion used for the \\nfunctions, respectively.Each predicted class probability is compared to the real class \\nintended result, which is either 0 or 1, to determine a score/loss that penalises the \\nprobability based on how far it deviates from the actual expect ed value.Data loader, \\noptimizer, criterion, model, and train data are used to produce a function called \"fit\" for \\ntraining.The parameters data loader, optimizer, criterion, model, and valid data are \\nsupplied to another validation function called \"validate. \"The validate loss and accuracy \\nare calculated using this function.Learning rate = 0.001, batch size = 128, and epochs = \\n10 are specified as learning parameters. Within the period range, \"fit\" is called to \\nprovide accuracy and loss for training.The model i s retained for future use after it has \\nbeen trained.To open the web camera and then show the gesture, which is anticipated, \\nand the word is displayed on the screen to receive the output and truly predict a hand \\nsign.  \\n \\n \\nFigure 6: Inception V3 Architecture  \\n \\n \\n \\n \\n \\n \\n \\n27 \\n 3.3 Flowchart  \\n \\n \\nFigure 7 : Flowchart  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n28 \\n CHAPTER 5 – RESULTS AND DISCUSSIONS;PERFORMANCE ANALYSIS  \\n \\n \\n \\n5.1 Training and Validating  \\n \\nThe data is trained using the CPU. As a result, CUDA is identified as zero (without \\nGPU). The 42000 dataset is split into two parts: one for training and the other for \\nvalidation.Training set: is a collection of samples used to learn how to fit the classif ier\\'s \\nparameters [i.e., weights]. Validation set: A set of examples used to fine -tune a \\nclassifier\\'s parameters [architecture, not weights], such as the number of hidden units in \\na neural network.  \\n \\n \\nFigure 8 : Number of files  \\nThe CustomCNN function create d is displayed below, which will be combined with the \\nInception Model to train the dataset.  \\n \\n \\nFigure 8: Custom CNN Architecture  \\n29 \\n  \\nThe model is then fitted with two functions: one for training and the other for validating. This will \\nproduce the results, which will be compared to the target.  \\n \\n \\n5.2 Results              \\n \\nGraph for train loss which shows the deviation of error loss while t raining. A loss \\nfunction, also known as a cost function, is a function that transfers an event or the \\nvalues of one or more variables onto a real number that intuitively represents some \\n\"cost\" connected with the event in mathematical optimization and decis ion theory. This \\nis the initial data epochs vs error loss. An epoch is a word used in machine learning that \\nrefers to the number of passes the machine learning algorithm has made across the full \\ntraining dataset. At 0 epoch, loss calculated is approx. 0.00 5. \\n \\nFigure 9. Train Loss  \\n \\n30 \\n The graph below depicts the initial training accuracy, which indicates that the model was \\nsaved after a checkpoint. At 0 epoch, accuracy attained is approx 83%.As the training \\nmoves forward it changes to 98%.  \\n \\nFigure 10. Training Accuracy  \\n \\n \\n \\n31 \\n  \\n       \\n \\n \\n \\n32 \\n  \\n \\n \\n: \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n33 \\n CHAPTER 6 – CONCLUSION AND FUTURE WORK  \\n \\n \\n6.1 Conclusion  \\n \\nUsing a CustomCNN classifier, the output is a fully trained model with a 93 percent \\naccuracy. With a total of 1200 samples, the visuals used were in Indian Sign Language. \\nThese photographs are organised into folders based on their meaning. The dataset was \\ndownloaded in jpeg format, then label binarized and pre -processed. We achieved a loss \\nrate of roughly 0.10 after training the model. After that, the trained model was saved and \\nreloaded. A script was used to enable the web camera. Finally, a hand gesture w as \\ndemonstrated, which was effectively detected.  \\n \\n \\n6.2 Future Work  \\n \\nIn the future, this might be modified into an application where a person practicing sign \\nlanguage can transmit live video to the other end, which can be understood by the \\nperson watching. Basically, when the video is sequenced, it will capture the main \\ngesture which classifies as the sign language, and then the recognized sign is \\nclassified. The database can be modified by adding more images to it. This will lead to \\nmaking the system more accurate in detecting everything.  \\n \\n \\n \\n \\n \\n  34 \\n REFERENCES  \\n \\n \\nNumPy, Pandas, MatplotLib: GeekForGeeks: geekforgeeks.org  \\n \\nPython3: www.python.org/download/releases/3.0/  \\n \\nMachine Learning : towardsdatascience.com  \\n \\n \\n[1] Study of vision -based hand gesture recognition using Indian sign language in \\nINTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS \\nVOL. 7, NO. 1, MARCH 2014.  \\n[2] Real-time hand gesture recognition using finger segmentation in THE SCIENTIFIC \\nWORLD JOURNAL (JAN 2014).  \\n[3] Hand sign recognition from depth images with multi -scale density features for deaf -\\nmute persons in International Conference on Computational Intelligence and Data \\nScience (ICCIDS 2019).  \\n \\n[4] Indian sign language interpreter using image processing and machine learning in \\nIOP Conference Series: Materials Science and Engineering 2020.  \\n \\n[5] Dynamic Hand Gesture Recognition: A Literature Review in IJERT 2012.  \\n \\n[6] Hand Gesture Recognition A Literature Review in IJSRD - International Journal for \\nScientific Research & Development| Vol. 8, Issue 2, 2020.  \\n[7] Tracking of dynamic gesture fingertips position in a video sequence in Archives of \\nControl Sciences Volume 30(LX VI), 2020.  \\n  [8] Visual tracking utilizing robust complementary learner and adaptive refiner in \\nScience   Direct,10th May 2017  35 \\n [9] Tracking of dynamic gesture fingertips position in a video sequence in Archives of \\nControl Sciences Volume 30(LXVI), 2020.  \\n[10] Visual tracking utilizing robust complementary learner and adaptive refiner in \\nScience Direct,10th May 2017.  \\n[11] Sign Language Recognition Using Image Processing, Research Gate \\n2017.Development of the Recognition System of Japanese Sign Language Using  3D \\nImage Sensor (HCI International 2013).  \\n \\n \\n \\n \\n ')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting it into documents\n",
    "from langchain.docstore.document import Document\n",
    "docs = [Document(page_content=text)]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Write a concise and short summary of the following speech.\n",
    "speech = `{text}`\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['text'],\n",
    "    template = template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type='stuff',\n",
    "    prompt = prompt,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_summary = chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Summarize the following speech: The goal of this project was to build a neural network \\nable to classify which word of the Indian Sign Language (ISL) is being signed, given an \\nimage of a signing hand. This project is a first step towards building a possible sign \\nlanguage translator, which  can take communications in sign language and translate them \\ninto written and oral language. Such a translator would greatly lower the barrier for many \\ndeaf and mute individuals to be able to better communicate with others in \\nday-to-day interactions.  \\n \\nThe dataset was mostly built utilising a live video feed to capture all of the signs \\nfrom A to Z and 1 to 9 in jpeg format. Some experimental data has been acquired by \\ninfluencing and changing the data. It achieves the study's goal of recognising Indian \\nsign language from a video feed.  \\n \\nUsing a CustomCNN classifier, the output is a fully trained model with a 93 percent \\naccuracy. With a total of 1200 samples, the visuals used were in Indian Sign Language. \\nThese photographs are organised into folders based on their meaning. The dataset was \\ndownloaded in jpeg format, then label binarized and pre -processed. We achieved a loss \\nrate of roughly 0.10 after training the model. After that, the trained model was saved and \\nreloaded. A script was used to enable the web camera. Finally, a hand gesture w as \\ndemonstrated, which was effectively detected.  \\n \\nIn the future, this might be modified into an application where a person practicing sign \\nlanguage can transmit live video to the other end, which can be understood by the \\nperson watching. Basically, when the video is sequenced, it will capture the main \\ngesture which classifies as the sign language, and then the recognized sign is \\nclassified. The database can be modified by adding more images to it. This will lead to \\nmaking the system more accurate in detecting everything.\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing large documents using map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfreader = PdfReader('1822-b.e-cse-batchno-224.pdf')\n",
    "from typing_extensions import Concatenate\n",
    "# read text from pdf \n",
    "text = ''\n",
    "for i , page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7520"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the text \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap = 20)\n",
    "chunks = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = chain.run(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map reduce with custom prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_prompt = \"\"\" \n",
    "Please summarize the below speech:\n",
    "Speech: `{text}`\n",
    "Summary:\n",
    "\"\"\" \n",
    "\n",
    "map_prompt_template = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template = chunks_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combine_prompt = ''' \n",
    "\n",
    "provide a final summary of the entire spech with these important points. \n",
    "Add a  generic motivational title,\n",
    "start the precise summary with an introduction and provide the summary in number points for the speech.\n",
    "\n",
    "Speech: `{text}`\n",
    "\n",
    "'''\n",
    "\n",
    "final_combine_prompt_template = PromptTemplate(\n",
    "    input_variables = ['text'],\n",
    "    template =final_combine_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='map_reduce',\n",
    "    map_prompt = map_prompt_template,\n",
    "    combine_prompt = final_combine_prompt_template,\n",
    "    verbose=False\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43msummary_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ankita.ankita\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ankita.ankita\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\chains\\base.py:545\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    546\u001b[0m         _output_key\n\u001b[0;32m    547\u001b[0m     ]\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    551\u001b[0m         _output_key\n\u001b[0;32m    552\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ankita.ankita\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ankita.ankita\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\chains\\base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    376\u001b[0m }\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ankita.ankita\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\chains\\base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\ankita.ankita\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\chains\\base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    160\u001b[0m     )\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ankita.ankita\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:137\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    136\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 137\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    138\u001b[0m     docs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_keys\n\u001b[0;32m    139\u001b[0m )\n\u001b[0;32m    140\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32mc:\\Users\\ankita.ankita\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\chains\\combine_documents\\map_reduce.py:226\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    216\u001b[0m     docs: List[Document],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    220\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m     map_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     question_result_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key\n\u001b[0;32m    232\u001b[0m     result_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    233\u001b[0m         Document(page_content\u001b[38;5;241m=\u001b[39mr[question_result_key], metadata\u001b[38;5;241m=\u001b[39mdocs[i]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[0;32m    236\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ankita.ankita\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\chains\\llm.py:228\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list, callbacks)\u001b[0m\n\u001b[0;32m    226\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 228\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs})\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\ankita.ankita\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\chains\\llm.py:258\u001b[0m, in \u001b[0;36mLLMChain.create_outputs\u001b[1;34m(self, llm_result)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm_result: LLMResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     result \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;66;03m# Get the text of the top generated string.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m         {\n\u001b[0;32m    261\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse_result(generation),\n\u001b[0;32m    262\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation,\n\u001b[0;32m    263\u001b[0m         }\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m llm_result\u001b[38;5;241m.\u001b[39mgenerations\n\u001b[0;32m    265\u001b[0m     ]\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_final_only:\n\u001b[0;32m    267\u001b[0m         result \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: r[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Users\\ankita.ankita\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\chains\\llm.py:261\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm_result: LLMResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     result \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;66;03m# Get the text of the top generated string.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m         {\n\u001b[1;32m--> 261\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    262\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation,\n\u001b[0;32m    263\u001b[0m         }\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m llm_result\u001b[38;5;241m.\u001b[39mgenerations\n\u001b[0;32m    265\u001b[0m     ]\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_final_only:\n\u001b[0;32m    267\u001b[0m         result \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: r[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[1;32mc:\\Users\\ankita.ankita\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain_core\\output_parsers\\base.py:221\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: List[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m        Structured output.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse(\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "output = summary_chain.run(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RefineChain for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"1 \n",
      "  \n",
      "FINAL YEAR PROJECT REPORT  \n",
      "at \n",
      "Sathyabama Institute of Science and Technology (Deemed to be \n",
      "University)  \n",
      " \n",
      "Submitted in partial fulfilment of the requirements for the award of Bachelor of \n",
      "Engineering Degree in Computer Science and Engineering  \n",
      " \n",
      " \n",
      " \n",
      "                                                              Pitambara Awadhesh  \n",
      "(Reg. No. 38110406)  \n",
      "Reema Rose Toppo  \n",
      "(Reg. No. 38110458)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "DEPARTMENT OF COMPUTER SCIENCE AND ENGINEERING SCHOOL OF \n",
      "COMPUTING  \n",
      "SATHYABAMA INSTITUTE OF SCIENCE AND TECHNOLOGY JEPPIAAR NAGAR, \n",
      "RAJIV GANDHI SALAI,  \n",
      "CHENNAI – 600119, TAMILNADU  \n",
      "2 \n",
      " 3 \n",
      "                                    SATHYABAMA  \n",
      "                                                           INSTITUTE OF SCIENCE AND TECHNOLOGY  \n",
      "(DEEMED TO BE UNIVERSITY)  \n",
      "                                                 Accredited with Grade “A” by NAAC  \n",
      "                                                        (Established under Section 3 of UGC Act, 1956)\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Final year project report submitted by Pitambara Awadhesh and Reema Rose Toppo.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "JEPPIAAR NAGAR, RAJIV GANDHI SALAI, CHENNAI – 600119  \n",
      "www.sathyabamauniversity.ac.in  \n",
      " \n",
      "_________________________________________________________________________________  \n",
      "DEPARTMENT OF COMPUTER SCIENCE AND ENGINEERING  \n",
      " \n",
      "BONAFIDE CERTIFICATE  \n",
      " \n",
      "This is to certify that this Project Report is the bonafide work of Pitambara Awadhesh (38110406) and \n",
      "Reema Rose Toppo (38110458) who carried out the project entitled Indian Sign Language Recognition \n",
      "System to Help Mute and Deaf People under my supervision  from _____________ to ____________.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Internal Guide                       External Guide  \n",
      "(MS. DEEPA, DR. BEVISH)         (DR. BEVISH)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Head of the Department  \n",
      "(DR.L. LAKSHMANAN and DR.S. VIGNESHWARI)  \n",
      " \n",
      "Submitted for Viva -voce Examination held on ________________________________  \n",
      " \n",
      " \n",
      "Internal Examiner                                                                                         External Examiner  \n",
      " \n",
      " \n",
      " \n",
      "4\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_community.llms.google_palm.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n",
      "Retrying langchain_community.llms.google_palm.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Final year project report submitted by Pitambara Awadhesh and Reema Rose Toppo under the guidance of Ms. Deepa and Dr. Bevish.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "4 \n",
      "                                                           ACKNOWLEDGEMENT  \n",
      " \n",
      " \n",
      " \n",
      "I am pleased to acknowledge my sincere thanks to Board of Management of SATHYABAMA for \n",
      "their kind encouragement in doing this project and for completing it successfully. I am grateful to \n",
      "them.  \n",
      " \n",
      " \n",
      " \n",
      "I convey my thanks to Dr. T. Sasikala M.E., Ph.D., Dean, Sch ool of Computer Science and \n",
      "Technology and Dr. L. Lakshmanan, Head of the Department, Dept. of Computer Science and \n",
      "Technology for providing me necessary support and details at the right time during the progressive \n",
      "reviews.  \n",
      " \n",
      " \n",
      "I would like to express my sin cere and deep sense of gratitude to my Project Guide Dr. B. Bharathi \n",
      "for her valuable guidance, suggestions and constant encouragement paved way for the successful \n",
      "completion of my project work . \n",
      " \n",
      " \n",
      "I wish to express my thanks to all Teaching and Non -teachin g staff members of the Department of\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Final year project report submitted by Pitambara Awadhesh and Reema Rose Toppo under the guidance of Ms. Deepa and Dr. Bevish. Acknowledgement  \n",
      " \n",
      " \n",
      " \n",
      "I am pleased to acknowledge my sincere thanks to Board of Management of SATHYABAMA for \n",
      "their kind encouragement in doing this project and for completing it successfully. I am grateful to \n",
      "them.  \n",
      " \n",
      " \n",
      " \n",
      "I convey my thanks to Dr. T. Sasikala M.E., Ph.D., Dean, Sch ool of Computer Science and \n",
      "Technology and Dr. L. Lakshmanan, Head of the Department, Dept. of Computer Science and \n",
      "Technology for providing me necessary support and details at the right time during the progressive \n",
      "reviews.  \n",
      " \n",
      " \n",
      "I would like to express my sin cere and deep sense of gratitude to my Project Guide Dr. B. Bharathi \n",
      "for her valuable guidance, suggestions and constant encouragement paved way for the successful \n",
      "completion of my project work . \n",
      " \n",
      " \n",
      "I wish to express my thanks to all Teaching and Non -teachin g staff members of the Department of Computer Science and Technology for their help and support during the course of my project work.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Computer Science and Engineering who were helpful in many ways for the completion of the \n",
      "project.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  5 \n",
      " INDIAN SIGN LANGUAGE RECOGNITION SYSTEM TO HELP DEAF AND MUTE \n",
      "PEOPLE  \n",
      " \n",
      "Submitted in partial fulfillment of the requirements for the award of Bachelor of Engineering / \n",
      "Technology degree in Computer Science and Engineering (Specialisation of degree)  \n",
      " \n",
      "By \n",
      "Pitambara Awadhesh (38110406)  \n",
      "Reema Rose Toppo (38110458)  \n",
      " \n",
      " \n",
      " \n",
      "DEPARTMENT OF  COMPUTER SCIENCE AND ENGINEERING  \n",
      "SCHOOL OF COMPUTER SCIENCE AND ENGINEERING  \n",
      " \n",
      "SATHYABAMA  \n",
      "INSTITUTE OF SCIENCE AND TECHNOLOGY  \n",
      "(DEEMED TO BE UNIVERSITY)  \n",
      "Accredited with Grade “A” by NAAC  \n",
      "JEPPIAAR NAGAR, RAJIV GANDHI SALAI, CHENNAI - 600119  \n",
      " \n",
      "MAY - 2022 \n",
      "  \n",
      "6 \n",
      "                                             ABSTRACT  \n",
      " \n",
      " \n",
      " \n",
      " Everything has changed due to the COVID19 pandemic. We went from offline to \n",
      "internet mode in no time. Some people found it easy to adjust to this way of life, but\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Final year project report submitted by Pitambara Awadhesh and Reema Rose Toppo under the guidance of Ms. Deepa and Dr. Bevish. Acknowledgement  \n",
      " \n",
      " \n",
      " \n",
      "I am pleased to acknowledge my sincere thanks to Board of Management of SATHYABAMA for \n",
      "their kind encouragement in doing this project and for completing it successfully. I am grateful to \n",
      "them.  \n",
      " \n",
      " \n",
      " \n",
      "I convey my thanks to Dr. T. Sasikala M.E., Ph.D., Dean, Sch ool of Computer Science and \n",
      "Technology and Dr. L. Lakshmanan, Head of the Department, Dept. of Computer Science and \n",
      "Technology for providing me necessary support and details at the right time during the progressive \n",
      "reviews.  \n",
      " \n",
      " \n",
      "I would like to express my sin cere and deep sense of gratitude to my Project Guide Dr. B. Bharathi \n",
      "for her valuable guidance, suggestions and constant encouragement paved way for the successful \n",
      "completion of my project work . \n",
      " \n",
      " \n",
      "I wish to express my thanks to all Teaching and Non -teachin g staff members of the Department of Computer Science and Technology for their help and support during the course of my project work.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  5 \n",
      " INDIAN SIGN LANGUAGE RECOGNITION SYSTEM TO HELP DEAF AND MUTE \n",
      "PEOPLE  \n",
      " \n",
      "Submitted in partial fulfillment of the requirements for the award of Bachelor of Engineering / \n",
      "Technology degree in Computer Science and Engineering (Specialisation of degree)  \n",
      " \n",
      "By \n",
      "Pitambara Awadhesh (38110406)  \n",
      "Reema Rose Toppo (38110458)  \n",
      " \n",
      " \n",
      " \n",
      "DEPARTMENT OF  COMPUTER SCIENCE AND ENGINEERING  \n",
      "SCHOOL OF COMPUTER SCIENCE AND ENGINEERING  \n",
      " \n",
      "SATHYABAMA  \n",
      "INSTITUTE OF SCIENCE AND TECHNOLOGY  \n",
      "(DEEMED TO BE UNIVERSITY)  \n",
      "Accredited with Grade “A” by NAAC  \n",
      "JEPPIAAR NAGAR, RAJIV GANDHI SALAI, CHENNAI - 600119  \n",
      " \n",
      "MAY - 2022 \n",
      "  \n",
      "6 \n",
      "                                             ABSTRACT  \n",
      " \n",
      " \n",
      " \n",
      " Everything has changed due to the COVID19 pandemic. We went from offline to \n",
      "internet mode in no time. Some people found it easy to adjust to this way of life, but others \n",
      "struggled.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "many others with disabilities never did and still find it difficult to explain their ideas i n \n",
      "online meetings. To solve this problem, we have proposed a Convolutional Neural \n",
      "Network (CNN) based model for Indian sign language recognition. In our proposed \n",
      "method, a mute or deaf person can interact with the camera integrated into a computer \n",
      "and use gestures that will be recognized and converted to text for others to understand. \n",
      "For this, we have created a sample dataset and pre -processed it using a label binarizer. \n",
      "Afterward, feature extraction was done using two models, first for the palm region and  \n",
      "the other for the fingers. Later on, this dataset was fed into a custom -made CNN model \n",
      "whose learning parameters were provided as 0.001 learning rate, 128 batch -size, and \n",
      "10 epochs. The model performed well with 93% of accuracy while recognizing a hand \n",
      "gesture. Hence, our proposed model can be integrated into other online meeting sites \n",
      "for the target audience to use.   7\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Final year project report submitted by Pitambara Awadhesh and Reema Rose Toppo under the guidance of Ms. Deepa and Dr. Bevish. Acknowledgement  \n",
      " \n",
      " \n",
      " \n",
      "I am pleased to acknowledge my sincere thanks to Board of Management of SATHYABAMA for \n",
      "their kind encouragement in doing this project and for completing it successfully. I am grateful to \n",
      "them.  \n",
      " \n",
      " \n",
      " \n",
      "I convey my thanks to Dr. T. Sasikala M.E., Ph.D., Dean, Sch ool of Computer Science and \n",
      "Technology and Dr. L. Lakshmanan, Head of the Department, Dept. of Computer Science and \n",
      "Technology for providing me necessary support and details at the right time during the progressive \n",
      "reviews.  \n",
      " \n",
      " \n",
      "I would like to express my sin cere and deep sense of gratitude to my Project Guide Dr. B. Bharathi \n",
      "for her valuable guidance, suggestions and constant encouragement paved way for the successful \n",
      "completion of my project work . \n",
      " \n",
      " \n",
      "I wish to express my thanks to all Teaching and Non -teachin g staff members of the Department of Computer Science and Technology for their help and support during the course of my project work.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  5 \n",
      " INDIAN SIGN LANGUAGE RECOGNITION SYSTEM TO HELP DEAF AND MUTE \n",
      "PEOPLE  \n",
      " \n",
      "Submitted in partial fulfillment of the requirements for the award of Bachelor of Engineering / \n",
      "Technology degree in Computer Science and Engineering (Specialisation of degree)  \n",
      " \n",
      "By \n",
      "Pitambara Awadhesh (38110406)  \n",
      "Reema Rose Toppo (38110458)  \n",
      " \n",
      " \n",
      " \n",
      "DEPARTMENT OF  COMPUTER SCIENCE AND ENGINEERING  \n",
      "SCHOOL OF COMPUTER SCIENCE AND ENGINEERING  \n",
      " \n",
      "SATHYABAMA  \n",
      "INSTITUTE OF SCIENCE AND TECHNOLOGY  \n",
      "(DEEMED TO BE UNIVERSITY)  \n",
      "Accredited with Grade “A” by NAAC  \n",
      "JEPPIAAR NAGAR, RAJIV GANDHI SALAI, CHENNAI - 600119  \n",
      " \n",
      "MAY - 2022 \n",
      "  \n",
      "6 \n",
      "                                             ABSTRACT  \n",
      " \n",
      " \n",
      " \n",
      "Due to the COVID19 pandemic, many people had to switch to online meetings. However, \n",
      "many others with disabilities never did and still find it difficult to explain their ideas i n \n",
      "online meetings. To solve this problem, we have proposed a Convolutional Neural \n",
      "Network (CNN) based model for Indian sign language recognition. In our proposed \n",
      "method, a mute or deaf person can interact with the camera integrated into a computer \n",
      "and use gestures that will be recognized and converted to text for others to understand. \n",
      "For this, we have created a sample dataset and pre -processed it using a label binarizer. \n",
      "Afterward, feature extraction was done using two models, first for the palm region and  \n",
      "the other for the fingers. Later on, this dataset was fed into a custom -made CNN model \n",
      "whose learning parameters were provided as 0.001 learning rate, 128 batch -size, and \n",
      "10 epochs. The model performed well with 93% of accuracy while recognizing a hand \n",
      "gesture. Hence, our proposed model can be integrated into other online meeting sites \n",
      "for the target audience to use.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "CONTENTS  \n",
      "CHAPTERS  TITLE  PAGE  \n",
      " Bonafide  2 \n",
      " Declaration  3 \n",
      " Acknowledgement  4 \n",
      " Certificate  5 \n",
      " Abstract  6 \n",
      "   \n",
      "1 INTRODUCTION   \n",
      "   \n",
      "1.1 Basics  8 \n",
      "1.2 Python3  8 \n",
      "1.3 Python Modules  9-11 \n",
      "1.4 Deep Learning and Types  12-13 \n",
      "1.5 OpenCV  14 \n",
      "1.6 Tensorflow  15 \n",
      "   \n",
      "2 Literature Survey   \n",
      "2.1 Civil airline fare prediction with a multi -attribute dual -\n",
      "stage attention mechanism  16 \n",
      "2.2 AirFare Prediction  17 \n",
      "   \n",
      "3 Materials and Methods used   \n",
      "3.1 EDA 18 \n",
      "3.2 Data -sets 18 \n",
      "3.3 Proposed Methodology and flowchart  19 \n",
      "3.4 Creating WebApp  20 \n",
      "   \n",
      "4 Results and Conclusions   \n",
      "4.1 Categorical data Graphs  21 \n",
      "4.2 Graphs representing feature selection  22-23 \n",
      "4.3 Results and estimation  24 \n",
      "   \n",
      "5 Summary and Future work  25 \n",
      " References  27 \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   8 \n",
      " CHAPTER 1 - INTRODUCTION  \n",
      " \n",
      " \n",
      "1.1 Basics  \n",
      " \n",
      "The goal of this project was to build a neural network able to classify which word of the Indian\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The final year project report submitted by Pitambara Awadhesh and Reema Rose Toppo under the guidance of Ms. Deepa and Dr. Bevish. Acknowledgement  \n",
      " \n",
      " \n",
      " \n",
      "I am pleased to acknowledge my sincere thanks to Board of Management of SATHYABAMA for \n",
      "their kind encouragement in doing this project and for completing it successfully. I am grateful to \n",
      "them.  \n",
      " \n",
      " \n",
      " \n",
      "I convey my thanks to Dr. T. Sasikala M.E., Ph.D., Dean, Sch ool of Computer Science and \n",
      "Technology and Dr. L. Lakshmanan, Head of the Department, Dept. of Computer Science and \n",
      "Technology for providing me necessary support and details at the right time during the progressive \n",
      "reviews.  \n",
      " \n",
      " \n",
      "I would like to express my sin cere and deep sense of gratitude to my Project Guide Dr. B. Bharathi \n",
      "for her valuable guidance, suggestions and constant encouragement paved way for the successful \n",
      "completion of my project work . \n",
      " \n",
      " \n",
      "I wish to express my thanks to all Teaching and Non -teachin g staff members of the Department of Computer Science and Technology for their help and support during the course of my project work.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  5 \n",
      " INDIAN SIGN LANGUAGE RECOGNITION SYSTEM TO HELP DEAF AND MUTE \n",
      "PEOPLE  \n",
      " \n",
      "Submitted in partial fulfillment of the requirements for the award of Bachelor of Engineering / \n",
      "Technology degree in Computer Science and Engineering (Specialisation of degree)  \n",
      " \n",
      "By \n",
      "Pitambara Awadhesh (38110406)  \n",
      "Reema Rose Toppo (38110458)  \n",
      " \n",
      " \n",
      " \n",
      "DEPARTMENT OF  COMPUTER SCIENCE AND ENGINEERING  \n",
      "SCHOOL OF COMPUTER SCIENCE AND ENGINEERING  \n",
      " \n",
      "SATHYABAMA  \n",
      "INSTITUTE OF SCIENCE AND TECHNOLOGY  \n",
      "(DEEMED TO BE UNIVERSITY)  \n",
      "Accredited with Grade “A” by NAAC  \n",
      "JEPPIAAR NAGAR, RAJIV GANDHI SALAI, CHENNAI - 600119  \n",
      " \n",
      "MAY - 2022 \n",
      "  \n",
      "6 \n",
      "                                             ABSTRACT  \n",
      " \n",
      " \n",
      " \n",
      "Due to the COVID19 pandemic, many people had to switch to online meetings. However, \n",
      "many others with disabilities never did and still find it difficult to explain their ideas i n \n",
      "online meetings. To solve this problem, we have proposed a Convolutional Neural \n",
      "Network (CNN) based model for Indian sign language recognition. In our proposed \n",
      "method, a mute or deaf person can interact with the camera integrated into a computer \n",
      "and use gestures that will be recognized and converted to text for others to understand. \n",
      "For this, we have created a sample dataset and pre -processed it using a label binarizer. \n",
      "Afterward, feature extraction was done using two models, first for the palm region and  \n",
      "the other for the fingers. Later on, this dataset was fed into a custom -made CNN model \n",
      "whose learning parameters were provided as 0.001 learning rate, 128 batch -size, and \n",
      "10 epochs. The model performed well with 93% of accuracy while recognizing a hand \n",
      "gesture. Hence, our proposed model can be integrated into other online meeting sites \n",
      "for the target audience to use.\n",
      "The final year project report submitted by Pitambara Awadhesh and Reema Rose Toppo under the guidance of Ms. Deepa and Dr. Bevish. Acknowledgement  \n",
      " \n",
      " \n",
      " \n",
      "I am pleased to acknowledge my sincere thanks to Board of Management of SATHYABAMA for \n",
      "their kind encouragement in doing this project and for completing it successfully. I am grateful to \n",
      "them.  \n",
      " \n",
      " \n",
      " \n",
      "I convey my thanks to Dr. T. Sasikala M.E., Ph.D., Dean, Sch ool of Computer Science and \n",
      "Technology and Dr. L. Lakshmanan, Head of the Department, Dept. of Computer Science and \n",
      "Technology for providing me necessary support and details at the right time during the progressive \n",
      "reviews.  \n",
      " \n",
      " \n",
      "I would like to express my sin cere and deep sense of gratitude to my Project Guide Dr. B. Bharathi \n",
      "for her valuable guidance, suggestions and constant encouragement paved way for the successful \n",
      "completion of my project work . \n",
      " \n",
      " \n",
      "I wish to express my thanks to all Teaching and Non -teachin g staff members of the Department of Computer Science and Technology for\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Sign Language (ISL) is being signed, given an image of a signing hand. This project is a first step \n",
      "towards building a possible sign language translator, which  can take communications in sign \n",
      "language and translate them into written and oral language. Such a translator would greatly lower \n",
      "the barrier for many deaf and mute individuals to be able to better communicate with others in \n",
      "day-to-day interactions.  \n",
      "This goal is further motivated by the isolation that is felt within the deaf community. Loneliness \n",
      "and depression exist at higher rates among the deaf population, especially when they are \n",
      "immersed in a hearing world. Large barriers that profoundly affect life quality stem from the \n",
      "communication disconnect between the deaf and the hearing. Some examples are information \n",
      "deprivation, limitation of social connections, and difficulty integrating in society.  \n",
      "Most research implementations for this task have used dept h maps generated by the depth\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The final year project report submitted by Pitambara Awadhesh and Reema Rose Toppo under the guidance of Ms. Deepa and Dr. Bevish. Acknowledgement  \n",
      " \n",
      " \n",
      " \n",
      "I am pleased to acknowledge my sincere thanks to Board of Management of SATHYABAMA for \n",
      "their kind encouragement in doing this project and for completing it successfully. I am grateful to \n",
      "them.  \n",
      " \n",
      " \n",
      " \n",
      "I convey my thanks to Dr. T. Sasikala M.E., Ph.D., Dean, Sch ool of Computer Science and \n",
      "Technology and Dr. L. Lakshmanan, Head of the Department, Dept. of Computer Science and \n",
      "Technology for providing me necessary support and details at the right time during the progressive \n",
      "reviews.  \n",
      " \n",
      " \n",
      "I would like to express my sin cere and deep sense of gratitude to my Project Guide Dr. B. Bharathi \n",
      "for her valuable guidance, suggestions and constant encouragement paved way for the successful \n",
      "completion of my project work . \n",
      " \n",
      " \n",
      "I wish to express my thanks to all Teaching and Non -teachin g staff members of the Department of Computer Science and Technology for their help and support during the course of my project work.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  5 \n",
      " INDIAN SIGN LANGUAGE RECOGNITION SYSTEM TO HELP DEAF AND MUTE \n",
      "PEOPLE  \n",
      " \n",
      "Submitted in partial fulfillment of the requirements for the award of Bachelor of Engineering / \n",
      "Technology degree in Computer Science and Engineering (Specialisation of degree)  \n",
      " \n",
      "By \n",
      "Pitambara Awadhesh (38110406)  \n",
      "Reema Rose Toppo (38110458)  \n",
      " \n",
      " \n",
      " \n",
      "DEPARTMENT OF  COMPUTER SCIENCE AND ENGINEERING  \n",
      "SCHOOL OF COMPUTER SCIENCE AND ENGINEERING  \n",
      " \n",
      "SATHYABAMA  \n",
      "INSTITUTE OF SCIENCE AND TECHNOLOGY  \n",
      "(DEEMED TO BE UNIVERSITY)  \n",
      "Accredited with Grade “A” by NAAC  \n",
      "JEPPIAAR NAGAR, RAJIV GANDHI SALAI, CHENNAI - 600119  \n",
      " \n",
      "MAY - 2022 \n",
      "  \n",
      "6 \n",
      "                                             ABSTRACT  \n",
      " \n",
      " \n",
      " \n",
      "Due to the COVID19 pandemic, many people had to switch to online meetings. However, \n",
      "many others with disabilities never did and still find it difficult to explain their ideas i n \n",
      "online meetings. To solve this problem, we have proposed a Convolutional Neural \n",
      "Network (CNN) based model for Indian sign language recognition. In our proposed \n",
      "method, a mute or deaf person can interact with the camera integrated into a computer \n",
      "and use gestures that will be recognized and converted to text for others to understand. \n",
      "For this, we have created a sample dataset and pre -processed it using a label binarizer. \n",
      "Afterward, feature extraction was done using two models, first for the palm region and  \n",
      "the other for the fingers. Later on, this dataset was fed into a custom -made CNN model \n",
      "whose learning parameters were provided as 0.001 learning rate, 128 batch -size, and \n",
      "10 epochs. The model performed well with 93% of accuracy while recognizing a hand \n",
      "gesture. Hence, our proposed model can be integrated into other online meeting sites \n",
      "for the target audience to use.\n",
      "This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "camera and high -resolution images. The objective of this project was to see if neural networks are \n",
      "able to classify signed ISL letters using an input video sequence of a person taken with a personal \n",
      "device such as a laptop web cam. This is in alignment with the motivation as this would make a \n",
      "future implementation of a real -time ISL -to-oral/written language translator practical in everyday \n",
      "situations.  \n",
      " \n",
      "1.2  Python3  \n",
      " \n",
      "Python is an interpreted, high, general -purpose programming language . Created by Guido \n",
      "van Rossum and first released in 1991, Python's design philosophy emphasizes code \n",
      "readability with its notable use of significant whitespace . Its language constructs and \n",
      "object -oriented  approach aim to help programmers write clear, logical code for small and 9 \n",
      " large -scale projects. Python is dynamically typed and garbage -collected . It supports \n",
      "multiple programming paradigms  including structured (particularly, procedural), object -\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "The model was implemented using Python 3 and Keras. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, Theano, or PlaidML. It was developed with a focus on enabling fast experimentation. Keras is designed to be modular and extensible, and supports both convolutional networks and recurrent neural networks.\n",
      "\n",
      "The model was trained on a dataset of 1000 images of hand gestures, which were collected from the Internet. The images were divided into a training set and a test set. The training set was used to train the model, and the test set was used to evaluate the model's performance. The model was able to achieve an accuracy of 93% on the test set.\n",
      "\n",
      "The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people. This would allow deaf and mute people to communicate more easily with others in online meetings.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "oriented, and functional programming. Python is often  described as a \"batteries included\" \n",
      "language due to its comprehensive standard library.  \n",
      " \n",
      " \n",
      "1.3 Analysis and Visualization of Data  \n",
      " \n",
      " \n",
      "(a) NumPy  \n",
      " \n",
      "NumPy is a python library used for working with arrays. It also has functions for \n",
      "working in  the domain of linear algebra, Fourier  transform, and matrices. NumPy \n",
      "was created in 2005 by Travis Oliphant. It is an open -source  project and you can \n",
      "use it freely. NumPy stands for Numerical Python.  \n",
      " \n",
      " \n",
      "Why NumPy?  \n",
      "In Python , we have lists that serve the \n",
      "purpose of arrays, but they are slow to \n",
      "process. NumPy aims to provide an \n",
      "array object that is up to 50x faster than \n",
      "traditional Python lists.  \n",
      " \n",
      "The array object in NumPy is called \n",
      "ndarray, in  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Figure 1: NumPy\n",
      "provides a lot of supporting functions that make working with ndarray very easy. Arrays \n",
      "are very frequently used in data science, where speed and resources are very \n",
      "important.  \n",
      " \n",
      " \n",
      " \n",
      "(b) Pandas\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "The model was implemented using Python 3 and Keras. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, Theano, or PlaidML. It was developed with a focus on enabling fast experimentation. Keras is designed to be modular and extensible, and supports both convolutional networks and recurrent neural networks.\n",
      "\n",
      "The model was trained on a dataset of 1000 images of hand gestures, which were collected from the Internet. The images were divided into a training set and a test set. The training set was used to train the model, and the test set was used to evaluate the model's performance. The model was able to achieve an accuracy of 93% on the test set.\n",
      "\n",
      "The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people. This would allow deaf and mute people to communicate more easily with others in online meetings.\n",
      "\n",
      "The model was developed using Python 3, which is a general-purpose programming language that is designed for both object-oriented and functional programming. Python is often described as a \"batteries included\" language due to its comprehensive standard library.\n",
      "\n",
      "The model also uses NumPy and Pandas, which are two Python libraries that are commonly used for data analysis and visualization. NumPy is a library for working with arrays, and Pandas is a library for working with tabular data.\n",
      "\n",
      "The model was trained on a dataset of 1000 images of hand gestures, which were collected from the Internet. The images were divided into a training set and a test set. The training set was used to train the model, and the test set was used to evaluate the model's performance. The model was able to achieve an accuracy of 93% on the test set.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "(b) Pandas  \n",
      " \n",
      "10 \n",
      " Pandas is a high -level data manipulation tool developed by Wes McKinney. It is built \n",
      "on the Numpy package and its key data structure is called the DataFrame. \n",
      "DataFrames allow you to store and manipulate tabular data in rows of observations \n",
      "and columns of v ariables.11 \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Figure 2: Pandas: DataFrames  \n",
      " \n",
      " \n",
      "DataFrame object for data manipulation with integrated indexing.  \n",
      " \n",
      "▪ Tools for reading and writing data between in -memory data structures and different file formats.  \n",
      "▪ Data alignment and integrated handling of missing data.  \n",
      "▪ Reshaping and pivoting of data sets.  \n",
      "▪ Label -based slicing, fancy indexing, and subsetting of large data sets.  \n",
      "▪ Data structure column insertion and deletion.  \n",
      "▪ Group by engine allowing split -apply -combine operations on data sets.  \n",
      "▪ Data set merging and joining.  \n",
      "▪ Hierarchical axis indexing to work with high -dimensional data in a lower -dimensional data structure.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "The model was implemented using Python 3, Keras, NumPy, and Pandas. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, Theano, or PlaidML. It was developed with a focus on enabling fast experimentation. Keras is designed to be modular and extensible, and supports both convolutional networks and recurrent neural networks.\n",
      "\n",
      "NumPy is a library for working with arrays, and Pandas is a library for working with tabular data. NumPy is used to store the images of hand gestures in a multidimensional array, and Pandas is used to load the data into a DataFrame. The DataFrame is then used to train the CNN model.\n",
      "\n",
      "The model was trained on a dataset of 1000 images of hand gestures, which were collected from the Internet. The images were divided into a training set and a test set. The training set was used to train the model, and the test set was used to evaluate the model's performance. The model was able to achieve an accuracy of 93% on the test set.\n",
      "\n",
      "The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people. This would allow deaf and mute people to communicate more easily with others in online meetings.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "▪ Time series -functionality: Date range generation and frequency conversion, moving window \n",
      "statistics, moving window linear regressions, date shifting , and lagging.  \n",
      "▪ Provides data filtration.  \n",
      " \n",
      " \n",
      " \n",
      "(c)  MatPlotlib  \n",
      "12 \n",
      " Matplotlib is a comprehensive library for creating static, animated, and interactive \n",
      "visualizations in Python. Matplotlib produces publication -quality figures in a variety of \n",
      "hardcopy formats and interactive environments across platforms. Matplotlib can be  \n",
      "used in Python scripts, the Python and IPython shell, web application servers, and \n",
      "various graphical user interface toolkits.  \n",
      " \n",
      " \n",
      " \n",
      "Figure 3: Plots using Matplotlib  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "• Convenient views onto the overall structure of complex data -set\n",
      "13 \n",
      " • High-level abstractions for structuring multi -plot grids that let you easily \n",
      "build complex visualizations  \n",
      "• Concise control over matplotlib figure styling with several built -in themes\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_community.llms.google_palm.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "The model was implemented using Python 3, Keras, NumPy, and Pandas. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, Theano, or PlaidML. It was developed with a focus on enabling fast experimentation. Keras is designed to be modular and extensible, and supports both convolutional networks and recurrent neural networks.\n",
      "\n",
      "NumPy is a library for working with arrays, and Pandas is a library for working with tabular data. NumPy is used to store the images of hand gestures in a multidimensional array, and Pandas is used to load the data into a DataFrame. The DataFrame is then used to train the CNN model.\n",
      "\n",
      "The model was trained on a dataset of 1000 images of hand gestures, which were collected from the Internet. The images were divided into a training set and a test set. The training set was used to train the model, and the test set was used to evaluate the model's performance. The model was able to achieve an accuracy of 93% on the test set.\n",
      "\n",
      "The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people. This would allow deaf and mute people to communicate more easily with others in online meetings.\n",
      "\n",
      "The model was also evaluated using the following metrics:\n",
      "\n",
      "* Accuracy: The accuracy of the model was 93%. This means that the model was able to correctly identify the hand gestures in 93% of the cases.\n",
      "* Precision: The precision of the model was 95%. This means that the model was able to correctly identify the hand gestures that were actually present in the images.\n",
      "* Recall: The recall of the model was 91%. This means that the model was able to identify all of the hand gestures that were present in the images.\n",
      "\n",
      "The model was able to achieve high accuracy, precision, and recall, which indicates that it is a good model for recognizing Indian Sign Language.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "• Tools for choosing color palettes that faithfully reveal patterns in your data \n",
      " \n",
      " \n",
      "1.4 Deep Learning  \n",
      " \n",
      "Deep learning is a subset of  machine learning, which is essentially a neural network \n",
      "with three or more layers. These neural networks attempt to simulate the behavior  of the \n",
      "human brain —albeit far from matching its ability —allowing it to ―learn‖ from large \n",
      "amounts of data. While a neural network with a single layer can still make approximate \n",
      "predictions, additional hidden layers can help to optimize and refine for accu racy.  \n",
      "Deep learning drives much  artificial intelligence (AI)  applications and services that \n",
      "improve automation, performing analytical and physical tasks without human \n",
      "intervention.  \n",
      "Deep neural networks consist of multiple layers of interconnected nodes, ea ch building \n",
      "upon the previous layer to refine and optimize the prediction or categorization. This \n",
      "progression of computations through the network is called forward propagation. The\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "The model was implemented using Python 3, Keras, NumPy, and Pandas. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, Theano, or PlaidML. It was developed with a focus on enabling fast experimentation. Keras is designed to be modular and extensible, and supports both convolutional networks and recurrent neural networks.\n",
      "\n",
      "NumPy is a library for working with arrays, and Pandas is a library for working with tabular data. NumPy is used to store the images of hand gestures in a multidimensional array, and Pandas is used to load the data into a DataFrame. The DataFrame is then used to train the CNN model.\n",
      "\n",
      "The model was trained on a dataset of 1000 images of hand gestures, which were collected from the Internet. The images were divided into a training set and a test set. The training set was used to train the model, and the test set was used to evaluate the model's performance. The model was able to achieve an accuracy of 93% on the test set.\n",
      "\n",
      "The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people. This would allow deaf and mute people to communicate more easily with others in online meetings.\n",
      "\n",
      "The model was also evaluated using the following metrics:\n",
      "\n",
      "* Accuracy: The accuracy of the model was 93%. This means that the model was able to correctly identify the hand gestures in 93% of the cases.\n",
      "* Precision: The precision of the model was 95%. This means that the model was able to correctly identify the hand gestures that were actually present in the images.\n",
      "* Recall: The recall of the model was 91%. This means that the model was able to identify all of the hand gestures that were present in the images.\n",
      "\n",
      "The model was able to achieve high accuracy, precision, and recall, which indicates that it is a good model for recognizing Indian Sign Language.\n",
      "\n",
      "The model is based on deep learning, which is a subset of machine learning that uses neural networks with three or more layers. These neural networks attempt to simulate the behavior of the human brain, allowing them to learn from large amounts of data. Deep learning is used in a variety of applications, including image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "The model was implemented using Python 3, Keras, NumPy, and Pandas. Keras is a high-level neural networks API that makes it easy to build and train deep learning models. NumPy is a library for working with arrays, and Pandas is a library for working with tabular data.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "input and output layers of a deep neural network are called  visible  layers.  The input \n",
      "layer is where the deep learning model ingests the data for processing, and the output \n",
      "layer is where the final prediction or classification is made.  \n",
      "To state the layer in a clear manner:  \n",
      "1. Input layer – The input layer has input features in a dat aset that is known to us.  \n",
      "2. Hidden Layer – Hidden layer, just like we need to train the brain through hidden \n",
      "neurons.  \n",
      "3. Output layer – value that we want to classify.  14 \n",
      "  \n",
      " \n",
      "Fig 5: Types of Deep learning algorithms  \n",
      " \n",
      " \n",
      "There are many types of deep learning algorithms developed over the years but there \n",
      "are a few algorithms that are frequently used:  \n",
      "1. Artificial Neural Network:  \n",
      " \n",
      "An artificial Neural Network is the component of a computing system designed in such a \n",
      "way that the human brain analyses and makes a decision. Ann is the building block of\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "The model was implemented using Python 3, Keras, NumPy, and Pandas. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, Theano, or PlaidML. It was developed with a focus on enabling fast experimentation. Keras is designed to be modular and extensible, and supports both convolutional networks and recurrent neural networks.\n",
      "\n",
      "NumPy is a library for working with arrays, and Pandas is a library for working with tabular data. NumPy is used to store the images of hand gestures in a multidimensional array, and Pandas is used to load the data into a DataFrame. The DataFrame is then used to train the CNN model.\n",
      "\n",
      "The model was trained on a dataset of 1000 images of hand gestures, which were collected from the Internet. The images were divided into a training set and a test set. The training set was used to train the model, and the test set was used to evaluate the model's performance. The model was able to achieve an accuracy of 93% on the test set.\n",
      "\n",
      "The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people. This would allow deaf and mute people to communicate more easily with others in online meetings.\n",
      "\n",
      "The model was also evaluated using the following metrics:\n",
      "\n",
      "* Accuracy: The accuracy of the model was 93%. This means that the model was able to correctly identify the hand gestures in 93% of the cases.\n",
      "* Precision: The precision of the model was 95%. This means that the model was able to correctly identify the hand gestures that were actually present in the images.\n",
      "* Recall: The recall of the model was 91%. This means that the model was able to identify all of the hand gestures that were present in the images.\n",
      "\n",
      "The model was able to achieve high accuracy, precision, and recall, which indicates that it is a good model for recognizing Indian Sign Language.\n",
      "\n",
      "The model is based on deep learning, which is a subset of machine learning that uses neural networks with three or more layers. These neural networks attempt to simulate the behavior of the human brain, allowing them to learn from large amounts of data. Deep learning is used in a variety of applications, including image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "The model was implemented using Python 3, Keras, NumPy, and Pandas. Keras is a high-level neural networks API that makes it easy to build and train deep learning models. NumPy is a library for working with arrays, and Pandas is a library for working with tabular data.\n",
      "\n",
      "The input and output layers of a deep neural network are called  visible  layers.  The input \n",
      "layer is where the deep learning model ingests the data for processing, and the output \n",
      "layer is where the final prediction or classification is made.  \n",
      "To state the layer in a clear manner:  \n",
      "1. Input layer – The input layer has input features in a dat aset that is known to us.  \n",
      "2. Hidden Layer – Hidden layer, just like we need to train the brain through hidden \n",
      "neurons.  \n",
      "3. Output layer – value that we want to classify.  14 \n",
      "  \n",
      " \n",
      "Fig 5: Types of Deep learning algorithms  \n",
      " \n",
      " \n",
      "There are many types of deep learning algorithms developed over the years but there \n",
      "are a few algorithms that are frequently used:  \n",
      "1. Artificial Neural Network:  \n",
      " \n",
      "An artificial Neural Network is the component of a computing system designed in such a \n",
      "way that the human brain analyses and makes a decision. Ann is the building block of\n",
      "\n",
      "The original summary is a good summary of the project. The new context provides some additional information about the different types of deep learning algorithms that are used in the model. This information can be added to the summary to provide a more comprehensive overview of the project.\n",
      "\n",
      "The following is a revised summary that includes the new context:\n",
      "\n",
      "This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "deep learning and solves problems  that seem  impossible or very difficult to humans.  \n",
      "15 \n",
      " Artificial neural networks work like a human brain. The human brain has billions of \n",
      "neurons and each neuron is made up of a cell body that is responsible for computing \n",
      "information by carrying forward information towards hidden neurons and providing  the \n",
      "final Output.  \n",
      "                         \n",
      "    Fig 6: Artificial Neural Networks  \n",
      " \n",
      "The aim is to minimize the error by adjusting the weight and bias of the interconnection \n",
      "which is known as backpropagation. With the process of backpropagation, the \n",
      "difference between the desired output and actual output produces the least error.  \n",
      "2. Convoluti onal Neural Network  \n",
      " \n",
      "CNN is a supervised type of Deep learning, most preferable used in image recognition \n",
      "and computer vision. CNN has multiple layers that process and extract important \n",
      "features from the image.  Convolutional neural networks are composed of multiple layers\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: **Original Summary**\n",
      "\n",
      "This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "**Revised Summary**\n",
      "\n",
      "This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "The model is based on deep learning, which is a subset of machine learning that uses neural networks with three or more layers. These neural networks attempt to simulate the behavior of the human brain, allowing them to learn from large amounts of data. Deep learning is used in a variety of applications, including image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "The model was implemented using Python 3, Keras, NumPy, and Pandas. Keras is a high-level neural networks API that makes it easy to build and train deep learning models. NumPy is a library for working with arrays, and Pandas is a library for working with tabular data.\n",
      "\n",
      "The input and output layers of a deep neural network are called  visible  layers.  The input \n",
      "layer is where the deep learning model ingests the data for processing, and the output \n",
      "layer is where the final prediction or classification is made.  \n",
      "To state the layer in a clear manner:  \n",
      "1. Input layer – The input layer has input features in a dat aset that is known to us.  \n",
      "2. Hidden Layer – Hidden layer, just like we need to train the brain through hidden \n",
      "neurons.  \n",
      "3. Output layer – value that we want to classify.  14 \n",
      "  \n",
      " \n",
      "Fig 5: Types of Deep learning algorithms  \n",
      " \n",
      " \n",
      "There are many types of deep learning algorithms developed over the years but there \n",
      "are a few algorithms that are frequently used:  \n",
      "1. Artificial Neural Network:  \n",
      " \n",
      "An artificial Neural Network is the component of a computing system designed in such a \n",
      "way that the human brain analyses and makes a decision. Ann is the building block of\n",
      "\n",
      "The original summary is a good summary of the project. The new context provides some additional information about the different types of deep learning algorithms that are used in the model. This information can be added to the summary to provide a more comprehensive overview of the project.\n",
      "\n",
      "The following is a revised summary that includes the new context:\n",
      "\n",
      "This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "The model is based on deep learning, which is a subset of machine learning that uses neural networks with three or more layers. These neural networks attempt to simulate the behavior of the human brain, allowing them to learn from large amounts of data. Deep learning is used in a variety of applications, including image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "The model was implemented using Python 3, Keras, NumPy, and Pandas. Keras is a high-level neural networks API that makes it easy to build and train deep learning models. NumPy is a library for working with arrays, and Pandas is a library for working with tabular data.\n",
      "\n",
      "The input and output layers of a deep neural network are called  visible  layers.  The input \n",
      "layer is where the deep learning model ingests the data for processing, and the output \n",
      "layer is where the final prediction or classification is made.  \n",
      "To state the layer in a clear manner:  \n",
      "1\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "of artificial neurons. Artificial neurons, a rough imitation of their biological counterparts, \n",
      "are mathematical functions that calculate the weighted sum of multiple inputs and \n",
      "output  an activat ion value. When you input an image in a ConvNet, each layer \n",
      "generates several activation functions that are passed on to the next layer.  \n",
      "16 \n",
      " The first layer usually extracts basic features such as horizontal or diagonal edges. This \n",
      "output is passed on to the n ext layer which detects more complex features such as \n",
      "corners or combinational edges. As we move deeper into the network it can identify \n",
      "even more complex features such as objects, faces, etc . \n",
      " \n",
      "Fig 7: Working of a CNN  \n",
      " \n",
      "3. Recurrent Neural Networks (RNNs)  \n",
      " \n",
      "RNN is a type of supervised deep learning where the output from the previous step is \n",
      "fed as input to the current step. RNN deep learning algorithm is best suited for \n",
      "sequential data. RNN is most preferably used in image captioning, time -series analysis,\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The original summary is a good summary of the project. The new context provides some additional information about the different types of deep learning algorithms that are used in the model. This information can be added to the summary to provide a more comprehensive overview of the project.\n",
      "\n",
      "The following is a revised summary that includes the new context:\n",
      "\n",
      "This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "The model is based on deep learning, which is a subset of machine learning that uses neural networks with three or more layers. These neural networks attempt to simulate the behavior of the human brain, allowing them to learn from large amounts of data. Deep learning is used in a variety of applications, including image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "The model was implemented using Python 3, Keras, NumPy, and Pandas. Keras is a high-level neural networks API that makes it easy to build and train deep learning models. NumPy is a library for working with arrays, and Pandas is a library for working with tabular data.\n",
      "\n",
      "The input and output layers of a deep neural network are called  visible  layers.  The input \n",
      "layer is where the deep learning model ingests the data for processing, and the output \n",
      "layer is where the final prediction or classification is made.  \n",
      "To state the layer in a clear manner:  \n",
      "1. Input layer – The input layer has input features in a dat aset that is known to us.  \n",
      "2. Hidden Layer – Hidden layer, just like we need to train the brain through hidden \n",
      "neurons.  \n",
      "3. Output layer – value that we want to classify.  14 \n",
      "  \n",
      " \n",
      "Fig 5: Types of Deep learning algorithms  \n",
      " \n",
      " \n",
      "There are many types of deep learning algorithms developed over the years but there \n",
      "are a few algorithms that are frequently used:  \n",
      "1. Artificial Neural Network:  \n",
      " \n",
      "An artificial Neural Network is the component of a computing system designed in such a \n",
      "way that the human brain analyses and makes a decision. Ann is the building block of\n",
      "\n",
      "The original summary is a good summary of the project. The new context provides some additional information about the different types of deep learning algorithms that are used in the model. This information can be added to the summary to provide a more comprehensive overview of the project.\n",
      "\n",
      "The following is a revised summary that includes the new context:\n",
      "\n",
      "This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "The model is based on deep learning, which is a subset of machine learning that uses neural networks with three or more layers. These neural networks attempt to simulate the behavior of the human brain, allowing them to learn from large amounts of data. Deep learning is used in a variety of applications, including image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "The model was implemented using Python 3, Keras, NumPy, and Pandas. Keras is a high-level neural networks API that makes it easy to build and train deep learning models. NumPy is a library for working with arrays, and Pandas is a library for working with tabular data.\n",
      "\n",
      "The input and output layers of a deep neural network are called  visible  layers.  The input \n",
      "layer is where the deep learning model ingests the data for processing, and the output \n",
      "layer is where the final prediction or classification is made.  \n",
      "To state the layer in a clear manner:  \n",
      "1. Input layer – The input layer has input features in a dat aset that is known to us.  \n",
      "2. Hidden Layer – Hidden layer, just like we need to train the brain through hidden \n",
      "neurons.  \n",
      "3. Output layer – value that we want to classify.  14 \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "natural -language processing, handwriting recognition, and machine translation.  \n",
      "17 \n",
      "  \n",
      " \n",
      " \n",
      "Fig 8: Recurrent Neural Networks (RNNs)  \n",
      " \n",
      "The most vital feature of RNN is the Hidden state, which memorizes some information \n",
      "about a sequence. There are mainly 4 steps of how RNN works.  \n",
      "1. The output of the hidden state at t -1 is fed into input at time t.  \n",
      "2. Same way, the output at time t fed into the input at time t+1.  \n",
      "3. RNN can process inputs of any considerable length  \n",
      "4. The RNN computation depends on historical sequence data and the model size \n",
      "doesn’t increase with input size . \n",
      " \n",
      "1.5 OpenCV  \n",
      " \n",
      " \n",
      "OpenCV is a great tool for image processing and performing computer vision tasks. It is \n",
      "an open -source library that can be used to perform tasks like face detection, objection \n",
      "18 \n",
      " tracking, landmark detection, and much more. It supports multiple languages incl uding \n",
      "python, java C++. Although, For this article, we will be limiting to python only.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The original summary is a good summary of the project. The new context provides some additional information about the different types of deep learning algorithms that are used in the model. This information can be added to the summary to provide a more comprehensive overview of the project.\n",
      "\n",
      "The following is a revised summary that includes the new context:\n",
      "\n",
      "This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "The model is based on deep learning, which is a subset of machine learning that uses neural networks with three or more layers. These neural networks attempt to simulate the behavior of the human brain, allowing them to learn from large amounts of data. Deep learning is used in a variety of applications, including image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "The model was implemented using Python 3, Keras, NumPy, and Pandas. Keras is a high-level neural networks API that makes it easy to build and train deep learning models. NumPy is a library for working with arrays, and Pandas is a library for working with tabular data.\n",
      "\n",
      "The input and output layers of a deep neural network are called  visible  layers.  The input \n",
      "layer is where the deep learning model ingests the data for processing, and the output \n",
      "layer is where the final prediction or classification is made.  \n",
      "To state the layer in a clear manner:  \n",
      "1. Input layer – The input layer has input features in a dat aset that is known to us.  \n",
      "2. Hidden Layer – Hidden layer, just like we need to train the brain through hidden \n",
      "neurons.  \n",
      "3. Output layer – value that we want to classify.  14 \n",
      "\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "natural -language processing, handwriting recognition, and machine translation.  \n",
      "17 \n",
      "  \n",
      " \n",
      " \n",
      "Fig 8: Recurrent Neural Networks (RNNs)  \n",
      " \n",
      "The most vital feature of RNN is the Hidden state, which memorizes some information \n",
      "about a sequence. There are mainly 4 steps of how RNN works.  \n",
      "1. The output of the hidden state at t -1 is fed into input at time t.  \n",
      "2. Same way, the output at time t fed into the input at time t+1.  \n",
      "3. RNN can process inputs of any considerable length  \n",
      "4. The RNN computation depends on historical sequence data and the model size \n",
      "doesn’t increase with input size . \n",
      " \n",
      "1.5 OpenCV  \n",
      " \n",
      " \n",
      "OpenCV is a great tool for image processing and performing computer vision tasks. It is \n",
      "an open -source library that can be used to perform tasks like face detection, objection \n",
      "18 \n",
      " tracking, landmark detection, and much more. It supports multiple languages incl uding \n",
      "python, java C++. Although, For this article, we will be limiting to python only.\n",
      "------------\n",
      "Given the context, the following changes can be made to the original summary:\n",
      "\n",
      "* The first sentence can be changed to \"This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition.\"\n",
      "* The second sentence can be changed to \"The model is able to recognize the hand gestures of a deaf or mute person and convert them into text.\"\n",
      "* The third sentence can be changed to \"This would allow deaf and mute people to communicate more easily with others in online meetings.\"\n",
      "* The fourth sentence can be changed to \"The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images.\"\n",
      "* The fifth sentence can be changed to \"The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\"\n",
      "\n",
      "The following are the original and revised summaries:\n",
      "\n",
      "Original Summary:\n",
      "\n",
      "This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "The library is equipped with hundreds of useful functions and algorithms, which are all \n",
      "freely available to us. Some of these functions are really common and are used in \n",
      "almost every computer vision task. Whereas many of the functions are still unexplored \n",
      "and haven’t received much attention yet.  \n",
      "With the help of Open CV in python, it's possible to process images, videos easily and \n",
      "can extract useful information from the m, as there are lots of functions available. Some \n",
      "of the common applications are  \n",
      "  \n",
      "1. Image Processing:  \n",
      "Images can be read, written, shown, and processed with the OpenCV, which can \n",
      "generate a new image from that either by changing its shape, color, or extrac ting \n",
      "something useful from the given one and write into a new image.  \n",
      "  \n",
      "2. Face Detection:  \n",
      "Either from the live streaming using a web camera or from the locally stored \n",
      "videos/images utilizing Haar -Cascade Classifiers.  \n",
      " \n",
      "3. Face Recognition:\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Revised Summary:\n",
      "\n",
      "This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "OpenCV is a great tool for image processing and performing computer vision tasks. It is an open-source library that can be used to perform tasks like face detection, object tracking, landmark detection, and much more. It supports multiple languages including python, java C++. Although, for this article, we will be limiting to python only.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "It is followed by face detection from the videos using OpenCV by drawing \n",
      "bounding boxes i.e. rectangles and then model training using ML algorithms to \n",
      "recognize faces.  \n",
      " \n",
      " \n",
      " \n",
      "4. Object Detection:  19 \n",
      " Open CV along with the YOLO, an object detection algorithm can be used to \n",
      "detect objects from the image, videos either moving or stationary objects.  \n",
      " \n",
      "    \n",
      "  \n",
      "     \n",
      " \n",
      "1.6 TensorFlow  \n",
      " \n",
      " \n",
      "Fig 9: TensorFlow  \n",
      " \n",
      "TensorFlow is an open -source library developed by Google primarily for deep learning \n",
      "applications. It also supports traditional machine learning. TensorFlow was originally \n",
      "developed for large numerical computations without keeping deep learning in mind. \n",
      "However, it proved to be very useful for deep learning development as well, and \n",
      "therefore Google open -sourced it.  \n",
      "TensorFlow accepts data in the form of multi -dimensional arrays of higher dimensions \n",
      "called tensors. Multi -dimensional arrays are very handy in handling large amounts of \n",
      "data.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Revised Summary:\n",
      "\n",
      "This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "OpenCV is a great tool for image processing and performing computer vision tasks. It is an open-source library that can be used to perform tasks like face detection, object tracking, landmark detection, and much more. It supports multiple languages including python, java C++, and .NET. Although, for this article, we will be limiting to python only.\n",
      "\n",
      "Object detection is a computer vision task that involves finding and identifying objects in an image or video. OpenCV along with the YOLO, an object detection algorithm, can be used to detect objects from the image, videos either moving or stationary objects.\n",
      "\n",
      "TensorFlow is an open-source library developed by Google primarily for deep learning applications. It also supports traditional machine learning. TensorFlow was originally developed for large numerical computations without keeping deep learning in mind. However, it proved to be very useful for deep learning development as well, and therefore Google open-sourced it. TensorFlow accepts data in the form of multi-dimensional arrays of higher dimensions called tensors. Multi-dimensional arrays are very handy in handling large amounts of data.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "data.  \n",
      "TensorFlow works on the basis of data flow graphs that have nodes and edges. As the \n",
      "execution mechanism is in the form of graphs, it is much easier to execute TensorFlow \n",
      "code in a distributed manner across a cluster of compu ters while using GPUs.  \n",
      "20 \n",
      "  \n",
      "1.6.1 Tensorflow Object Detection API  \n",
      "Creating accurate machine learning models capable of localizing and identifying \n",
      "multiple objects in a single image remains a core challenge in computer vision. The \n",
      "TensorFlow Object Detection AP I is an open -source framework built on top of \n",
      "TensorFlow that makes it easy to construct, train and deploy object detection models.  \n",
      " \n",
      "Fig 10: Object Detection in Tensorflow  \n",
      " \n",
      " \n",
      " \n",
      "  \n",
      "21 \n",
      " CHAPTER 2 – LITERATURE SURVEY  \n",
      " \n",
      " \n",
      "2.1 Vision -based Real -Time Hand Gesture Recognition Techniques for Human -\n",
      "Computer Interaction (IEEE)  \n",
      " \n",
      "At first, we started studying the most basic human -computer interaction and how it\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Revised Summary:\n",
      "\n",
      "This project proposes a Convolutional Neural Network (CNN) based model for Indian Sign Language (ISL) recognition. The model is able to recognize the hand gestures of a deaf or mute person and convert them into text. This would allow deaf and mute people to communicate more easily with others in online meetings. The model was trained on a dataset of 1000 images of hand gestures, and it achieved an accuracy of 93% on a test set of 200 images. The model can be integrated into other online meeting sites to make them more accessible to deaf and mute people.\n",
      "\n",
      "OpenCV is a great tool for image processing and performing computer vision tasks. It is an open-source library that can be used to perform tasks like face detection, object tracking, landmark detection, and much more. It supports multiple languages including python, java C++, and .NET. Although, for this article, we will be limiting to python only.\n",
      "\n",
      "Object detection is a computer vision task that involves finding and identifying objects in an image or video. OpenCV along with the YOLO, an object detection algorithm, can be used to detect objects from the image, videos either moving or stationary objects.\n",
      "\n",
      "TensorFlow is an open-source library developed by Google primarily for deep learning applications. It also supports traditional machine learning. TensorFlow was originally developed for large numerical computations without keeping deep learning in mind. However, it proved to be very useful for deep learning development as well, and therefore Google open-sourced it. TensorFlow accepts data in the form of multi-dimensional arrays of higher dimensions called tensors. Multi-dimensional arrays are very handy in handling large amounts of data.\n",
      "\n",
      "The TensorFlow Object Detection API is an open-source framework built on top of TensorFlow that makes it easy to construct, train and deploy object detection models.\n",
      "\n",
      "In this paper, we propose a vision-based real-time hand gesture recognition technique for human-computer interaction. The proposed technique uses a Convolutional Neural Network (CNN) to extract features from hand gesture images and a Long Short-Term Memory (LSTM) to classify the hand gestures. The proposed technique is evaluated on the publicly available Sign Language MNIST dataset and achieves an accuracy of 93%.\n",
      "\n",
      "The proposed technique has several advantages over existing techniques. First, it is able to recognize hand gestures in real-time. Second, it is able to recognize hand gestures from different viewpoints. Third, it is able to recognize hand gestures that are partially occluded. Finally, the proposed technique is able to recognize hand gestures that are performed in different lighting conditions.\n",
      "\n",
      "The proposed technique can be used in a variety of applications, such as sign language recognition, human-computer interaction, and video surveillance.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "actually works. Ghotkar and Kharate explained three techniques and experime nted \n",
      "with interaction with a Desktop/Laptop with static hand gestures. All these techniques \n",
      "used a real -time approach with different feature descriptors such as Fourier \n",
      "Descriptor(FD),7 Hu moments, Convex Hull, and Finger Detection. Real -time \n",
      "Recognition e fficiency was calculated with respect to recognition time for FD and 7 Hu \n",
      "moments.  \n",
      " \n",
      "There are two major approaches for hand gesture recognition: Data Glove, Vision -\n",
      "based. Each approach is having its limitations and advantages, but vision -based \n",
      "approaches are more feasible as compared to data gloves as users do not need to \n",
      "wear cumbersome devices like data gloves.  \n",
      " \n",
      "Vision -based hand gesture recognition is having challenges such as variable lighting \n",
      "conditions, dynamic background, and skin color detection, considering these fact \n",
      "algorithms were developed to overcome some of these challenges.  Recognition of\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Revised Summary:\n",
      "\n",
      "This project proposes a vision-based real-time hand gesture recognition technique for human-computer interaction. The proposed technique uses a Convolutional Neural Network (CNN) to extract features from hand gesture images and a Long Short-Term Memory (LSTM) to classify the hand gestures. The proposed technique is evaluated on the publicly available Sign Language MNIST dataset and achieves an accuracy of 93%.\n",
      "\n",
      "The proposed technique has several advantages over existing techniques. First, it is able to recognize hand gestures in real-time. Second, it is able to recognize hand gestures from different viewpoints. Third, it is able to recognize hand gestures that are partially occluded. Finally, the proposed technique is able to recognize hand gestures that are performed in different lighting conditions.\n",
      "\n",
      "The proposed technique can be used in a variety of applications, such as sign language recognition, human-computer interaction, and video surveillance.\n",
      "\n",
      "The context provides additional information about the challenges of vision-based hand gesture recognition. It also provides an overview of existing techniques for hand gesture recognition. This information can be used to improve the proposed technique and make it more robust to challenges such as variable lighting conditions, dynamic background, and skin color detection.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "hand gestures executes windows applications such as opening Notepad, Windows \n",
      "media player, Internet Explorer, and MS -Paint. All algorithms were working on bare \n",
      "hands where the user need not wear any color gloves or data gloves . \n",
      " \n",
      " 22 \n",
      "   23 \n",
      "  \n",
      "2.2 Real-Time Hand Gesture Recognition Using Finger Segmentation (Hindawi)  \n",
      " \n",
      " \n",
      "In this paper, Zhi -Hua Chen and Jung -Tae Kim present an efficient and effective method \n",
      "for hand gesture recognition. The hand region is detected through the background \n",
      "subtraction method. Then, the palm and fingers are split so as to recognize the fingers. \n",
      "After the fingers are recognized, the hand gesture can be classified through a simple \n",
      "rule classifier.  \n",
      " \n",
      "Fig 11: Base paper methodology  \n",
      " \n",
      " \n",
      "2.3 Tracking of dynamic gesture fingertips position in a video sequence  \n",
      " \n",
      " \n",
      "The field of research of this paper combines Human -Computer Interface, gesture \n",
      "recognition, and fingertip tracking. Most gesture recognition algorithms processing color\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Revised Summary:\n",
      "\n",
      "This project proposes a vision-based real-time hand gesture recognition technique for human-computer interaction. The proposed technique uses a Convolutional Neural Network (CNN) to extract features from hand gesture images and a Long Short-Term Memory (LSTM) to classify the hand gestures. The proposed technique is evaluated on the publicly available Sign Language MNIST dataset and achieves an accuracy of 93%.\n",
      "\n",
      "The proposed technique has several advantages over existing techniques. First, it is able to recognize hand gestures in real-time. Second, it is able to recognize hand gestures from different viewpoints. Third, it is able to recognize hand gestures that are partially occluded. Finally, the proposed technique is able to recognize hand gestures that are performed in different lighting conditions.\n",
      "\n",
      "The proposed technique can be used in a variety of applications, such as sign language recognition, human-computer interaction, and video surveillance.\n",
      "\n",
      "The context provides additional information about the challenges of vision-based hand gesture recognition. It also provides an overview of existing techniques for hand gesture recognition. This information can be used to improve the proposed technique and make it more robust to challenges such as variable lighting conditions, dynamic background, and skin color detection.\n",
      "\n",
      "In particular, the context provides information about the following challenges:\n",
      "\n",
      "* **Variable lighting conditions:** Hand gesture recognition algorithms are often sensitive to changes in lighting conditions. This can make it difficult to recognize hand gestures in different lighting conditions. The proposed technique uses a CNN to extract features from hand gesture images. CNNs are invariant to changes in lighting conditions, which makes them well-suited for hand gesture recognition in different lighting conditions.\n",
      "* **Dynamic background:** Hand gesture recognition algorithms are often sensitive to the background in which the hand gestures are performed. This can make it difficult to recognize hand gestures that are performed in front of a cluttered or dynamic background. The proposed technique uses a LSTM to classify hand gestures. LSTMs are able to learn long-term dependencies, which makes them well-suited for hand gesture recognition in front of a cluttered or dynamic background.\n",
      "* **Skin color detection:** Hand gesture recognition algorithms often require the user to wear a colored glove or data glove in order to track the hand gestures. This can be inconvenient for the user and can also make it difficult to recognize hand gestures in front of a colored background. The proposed technique does not require the user to wear a colored glove or data glove. This makes it more convenient for the user and also makes it easier to recognize hand gestures in front of a colored background.\n",
      "\n",
      "The proposed technique addresses these challenges by using a CNN to extract features from hand gesture images and a LSTM to classify hand gestures. This makes the proposed technique more robust to changes in lighting conditions, dynamic background, and skin color detection.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "images are unable to locate folded fingers hidden inside hand contour. With the use of \n",
      "hand landmarks detection and localization alg orithm, processing directional images, the \n",
      "fingertips are tracked whether they are risen or folded inside the hand contour. The \n",
      "capabilities of the method, repeatability, and accuracy, are tested with the use of 3 \n",
      "gestures that are recorded on the USB came ra. Fingertips are tracked in gestures \n",
      "presenting a linear movement of an open hand, finger folding into a fist, and clenched \n",
      "fist movement. In conclusion, a discussion of accuracy in application to HCI is \n",
      "presented.  \n",
      " \n",
      "24 \n",
      "  \n",
      " \n",
      "2.4  \n",
      " \n",
      "  25 \n",
      " CHAPTER 4  -EXPERIMENTAL OR MATERIALS AND METHODS; ALGORITHMS USED  \n",
      " \n",
      " \n",
      " \n",
      "3.1 Exploratory Data Analysis  \n",
      " \n",
      "Confusion Matrix  \n",
      " \n",
      " \n",
      "3.2 Dataset  \n",
      " \n",
      "The dataset is prepared by enabling a webcam.  Labels should be binarized in a one -\n",
      "to-all fashion.Scikit -learn includes a number of regression and binary classification\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Revised Summary:\n",
      "\n",
      "This project proposes a vision-based real-time hand gesture recognition technique for human-computer interaction. The proposed technique uses a Convolutional Neural Network (CNN) to extract features from hand gesture images and a Long Short-Term Memory (LSTM) to classify the hand gestures. The proposed technique is evaluated on the publicly available Sign Language MNIST dataset and achieves an accuracy of 93%.\n",
      "\n",
      "The proposed technique has several advantages over existing techniques. First, it is able to recognize hand gestures in real-time. Second, it is able to recognize hand gestures from different viewpoints. Third, it is able to recognize hand gestures that are partially occluded. Finally, the proposed technique is able to recognize hand gestures that are performed in different lighting conditions.\n",
      "\n",
      "The proposed technique can be used in a variety of applications, such as sign language recognition, human-computer interaction, and video surveillance.\n",
      "\n",
      "The context provides additional information about the challenges of vision-based hand gesture recognition. It also provides an overview of existing techniques for hand gesture recognition. This information can be used to improve the proposed technique and make it more robust to challenges such as variable lighting conditions, dynamic background, and skin color detection.\n",
      "\n",
      "In particular, the context provides information about the following challenges:\n",
      "\n",
      "* **Variable lighting conditions:** Hand gesture recognition algorithms are often sensitive to changes in lighting conditions. This can make it difficult to recognize hand gestures in different lighting conditions. The proposed technique uses a CNN to extract features from hand gesture images. CNNs are invariant to changes in lighting conditions, which makes them well-suited for hand gesture recognition in different lighting conditions.\n",
      "* **Dynamic background:** Hand gesture recognition algorithms are often sensitive to the background in which the hand gestures are performed. This can make it difficult to recognize hand gestures that are performed in front of a cluttered or dynamic background. The proposed technique uses a LSTM to classify hand gestures. LSTMs are able to learn long-term dependencies, which makes them well-suited for hand gesture recognition in front of a cluttered or dynamic background.\n",
      "* **Skin color detection:** Hand gesture recognition algorithms often require the user to wear a colored glove or data glove in order to track the hand gestures. This can be inconvenient for the user and can also make it difficult to recognize hand gestures in front of a colored background. The proposed technique does not require the user to wear a colored glove or data glove. This makes it more convenient for the user and also makes it easier to recognize hand gestures in front of a colored background.\n",
      "\n",
      "The proposed technique addresses these challenges by using a CNN to extract features from hand gesture images and a LSTM to classify hand gestures. This makes the proposed technique more robust to changes in lighting conditions, dynamic background, and skin color detection.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "techniques. The so -called one -vs-all approach is a straightforward way to extend these \n",
      "algorithms to the multi -class classification p roblem.This basically entails learning one \n",
      "regressor or binary classifier per class throughout learning time. To do so, multi -class \n",
      "labels must be converted to binary labels (belong or do not belong to the class). The \n",
      "transform method in LabelBinarizer mak es this operation simple.When it comes to \n",
      "prediction, one assigns the class for which the matching model provided the highest \n",
      "level of confidence. The inverse transform method in LabelBinarizer makes this \n",
      "simple.The dataset was mostly built utilising a liv e video feed to capture all of the signs \n",
      "from A to Z and 1 to 9 in jpeg format.Some experimental data has been acquired by \n",
      "influencing and changing the data.It accomplishes the study's goal of recognising \n",
      "Indian sign language from a video feed.  \n",
      " \n",
      "3.3 Proposed Methodology\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Revised Summary:\n",
      "\n",
      "This project proposes a vision-based real-time hand gesture recognition technique for human-computer interaction. The proposed technique uses a Convolutional Neural Network (CNN) to extract features from hand gesture images and a Long Short-Term Memory (LSTM) to classify the hand gestures. The proposed technique is evaluated on the publicly available Sign Language MNIST dataset and achieves an accuracy of 93%.\n",
      "\n",
      "The proposed technique has several advantages over existing techniques. First, it is able to recognize hand gestures in real-time. Second, it is able to recognize hand gestures from different viewpoints. Third, it is able to recognize hand gestures that are partially occluded. Finally, the proposed technique is able to recognize hand gestures that are performed in different lighting conditions.\n",
      "\n",
      "The proposed technique can be used in a variety of applications, such as sign language recognition, human-computer interaction, and video surveillance.\n",
      "\n",
      "The context provides additional information about the challenges of vision-based hand gesture recognition. It also provides an overview of existing techniques for hand gesture recognition. This information can be used to improve the proposed technique and make it more robust to challenges such as variable lighting conditions, dynamic background, and skin color detection.\n",
      "\n",
      "In particular, the context provides information about the following challenges:\n",
      "\n",
      "* **Variable lighting conditions:** Hand gesture recognition algorithms are often sensitive to changes in lighting conditions. This can make it difficult to recognize hand gestures in different lighting conditions. The proposed technique uses a CNN to extract features from hand gesture images. CNNs are invariant to changes in lighting conditions, which makes them well-suited for hand gesture recognition in different lighting conditions.\n",
      "* **Dynamic background:** Hand gesture recognition algorithms are often sensitive to the background in which the hand gestures are performed. This can make it difficult to recognize hand gestures that are performed in front of a cluttered or dynamic background. The proposed technique uses a LSTM to classify hand gestures. LSTMs are able to learn long-term dependencies, which makes them well-suited for hand gesture recognition in front of a cluttered or dynamic background.\n",
      "* **Skin color detection:** Hand gesture recognition algorithms often require the user to wear a colored glove or data glove in order to track the hand gestures. This can be inconvenient for the user and can also make it difficult to recognize hand gestures in front of a colored background. The proposed technique does not require the user to wear a colored glove or data glove. This makes it more convenient for the user and also makes it easier to recognize hand gestures in front of a colored background.\n",
      "\n",
      "The proposed technique addresses these challenges by using a CNN to extract features from hand gesture images and a LSTM to classify hand gestures. This makes the proposed technique more robust to changes in lighting conditions, dynamic background, and skin color detection.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "CNN (Convolutional Neural Networks) is used in the suggested system. It's a four -layer \n",
      "modified CNN model that starts with three nodes.Modules like the torch were used to \n",
      "load, divide, and test the data during the training phase.The e pochs were set to ten, and \n",
      "the device was examined to determine whether Cuda was available; if it wasn't, the \n",
      "CPU was used.The photo positions and labels were afterwards obtained from the 26 \n",
      " data.csv file. Adam and Cross -Entropy Loss are the optimizer and cri terion used for the \n",
      "functions, respectively.Each predicted class probability is compared to the real class \n",
      "intended result, which is either 0 or 1, to determine a score/loss that penalises the \n",
      "probability based on how far it deviates from the actual expect ed value.Data loader, \n",
      "optimizer, criterion, model, and train data are used to produce a function called \"fit\" for \n",
      "training.The parameters data loader, optimizer, criterion, model, and valid data are\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Revised Summary:\n",
      "\n",
      "This project proposes a vision-based real-time hand gesture recognition technique for human-computer interaction. The proposed technique uses a Convolutional Neural Network (CNN) to extract features from hand gesture images and a Long Short-Term Memory (LSTM) to classify the hand gestures. The proposed technique is evaluated on the publicly available Sign Language MNIST dataset and achieves an accuracy of 93%.\n",
      "\n",
      "The proposed technique has several advantages over existing techniques. First, it is able to recognize hand gestures in real-time. Second, it is able to recognize hand gestures from different viewpoints. Third, it is able to recognize hand gestures that are partially occluded. Finally, the proposed technique is able to recognize hand gestures that are performed in different lighting conditions.\n",
      "\n",
      "The proposed technique can be used in a variety of applications, such as sign language recognition, human-computer interaction, and video surveillance.\n",
      "\n",
      "The context provides additional information about the challenges of vision-based hand gesture recognition. It also provides an overview of existing techniques for hand gesture recognition. This information can be used to improve the proposed technique and make it more robust to challenges such as variable lighting conditions, dynamic background, and skin color detection.\n",
      "\n",
      "In particular, the context provides information about the following challenges:\n",
      "\n",
      "* **Variable lighting conditions:** Hand gesture recognition algorithms are often sensitive to changes in lighting conditions. This can make it difficult to recognize hand gestures in different lighting conditions. The proposed technique uses a CNN to extract features from hand gesture images. CNNs are invariant to changes in lighting conditions, which makes them well-suited for hand gesture recognition in different lighting conditions.\n",
      "* **Dynamic background:** Hand gesture recognition algorithms are often sensitive to the background in which the hand gestures are performed. This can make it difficult to recognize hand gestures that are performed in front of a cluttered or dynamic background. The proposed technique uses a LSTM to classify hand gestures. LSTMs are able to learn long-term dependencies, which makes them well-suited for hand gesture recognition in front of a cluttered or dynamic background.\n",
      "* **Skin color detection:** Hand gesture recognition algorithms often require the user to wear a colored glove or data glove in order to track the hand gestures. This can be inconvenient for the user and can also make it difficult to recognize hand gestures in front of a colored background. The proposed technique does not require the user to wear a colored glove or data glove. This makes it more convenient for the user and also makes it easier to recognize hand gestures in front of a colored background.\n",
      "\n",
      "The proposed technique addresses these challenges by using a CNN to extract features from hand gesture images and a LSTM to classify hand gestures. This makes the proposed technique more robust to changes in lighting conditions, dynamic background, and skin color detection.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "supplied to another validation function called \"validate. \"The validate loss and accuracy \n",
      "are calculated using this function.Learning rate = 0.001, batch size = 128, and epochs = \n",
      "10 are specified as learning parameters. Within the period range, \"fit\" is called to \n",
      "provide accuracy and loss for training.The model i s retained for future use after it has \n",
      "been trained.To open the web camera and then show the gesture, which is anticipated, \n",
      "and the word is displayed on the screen to receive the output and truly predict a hand \n",
      "sign.  \n",
      " \n",
      " \n",
      "Figure 6: Inception V3 Architecture  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "27 \n",
      " 3.3 Flowchart  \n",
      " \n",
      " \n",
      "Figure 7 : Flowchart  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "28 \n",
      " CHAPTER 5 – RESULTS AND DISCUSSIONS;PERFORMANCE ANALYSIS  \n",
      " \n",
      " \n",
      " \n",
      "5.1 Training and Validating  \n",
      " \n",
      "The data is trained using the CPU. As a result, CUDA is identified as zero (without \n",
      "GPU). The 42000 dataset is split into two parts: one for training and the other for\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Revised Summary:\n",
      "\n",
      "This project proposes a vision-based real-time hand gesture recognition technique for human-computer interaction. The proposed technique uses a Convolutional Neural Network (CNN) to extract features from hand gesture images and a Long Short-Term Memory (LSTM) to classify the hand gestures. The proposed technique is evaluated on the publicly available Sign Language MNIST dataset and achieves an accuracy of 93%.\n",
      "\n",
      "The proposed technique has several advantages over existing techniques. First, it is able to recognize hand gestures in real-time. Second, it is able to recognize hand gestures from different viewpoints. Third, it is able to recognize hand gestures that are partially occluded. Finally, the proposed technique is able to recognize hand gestures that are performed in different lighting conditions.\n",
      "\n",
      "The proposed technique can be used in a variety of applications, such as sign language recognition, human-computer interaction, and video surveillance.\n",
      "\n",
      "The context provides additional information about the challenges of vision-based hand gesture recognition. It also provides an overview of existing techniques for hand gesture recognition. This information can be used to improve the proposed technique and make it more robust to challenges such as variable lighting conditions, dynamic background, and skin color detection.\n",
      "\n",
      "In particular, the context provides information about the following challenges:\n",
      "\n",
      "* **Variable lighting conditions:** Hand gesture recognition algorithms are often sensitive to changes in lighting conditions. This can make it difficult to recognize hand gestures in different lighting conditions. The proposed technique uses a CNN to extract features from hand gesture images. CNNs are invariant to changes in lighting conditions, which makes them well-suited for hand gesture recognition in different lighting conditions.\n",
      "* **Dynamic background:** Hand gesture recognition algorithms are often sensitive to the background in which the hand gestures are performed. This can make it difficult to recognize hand gestures that are performed in front of a cluttered or dynamic background. The proposed technique uses a LSTM to classify hand gestures. LSTMs are able to learn long-term dependencies, which makes them well-suited for hand gesture recognition in front of a cluttered or dynamic background.\n",
      "* **Skin color detection:** Hand gesture recognition algorithms often require the user to wear a colored glove or data glove in order to track the hand gestures. This can be inconvenient for the user and can also make it difficult to recognize hand gestures in front of a colored background. The proposed technique does not require the user to wear a colored glove or data glove. This makes it more convenient for the user and also makes it easier to recognize hand gestures in front of a colored background.\n",
      "\n",
      "The proposed technique addresses these challenges by using a CNN to extract features from hand gesture images and a LSTM to classify hand gestures. This makes the proposed technique more robust to changes in lighting conditions, dynamic background, and skin color detection.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "validation.Training set: is a collection of samples used to learn how to fit the classif ier's \n",
      "parameters [i.e., weights]. Validation set: A set of examples used to fine -tune a \n",
      "classifier's parameters [architecture, not weights], such as the number of hidden units in \n",
      "a neural network.  \n",
      " \n",
      " \n",
      "Figure 8 : Number of files  \n",
      "The CustomCNN function create d is displayed below, which will be combined with the \n",
      "Inception Model to train the dataset.  \n",
      " \n",
      " \n",
      "Figure 8: Custom CNN Architecture  \n",
      "29 \n",
      "  \n",
      "The model is then fitted with two functions: one for training and the other for validating. This will \n",
      "produce the results, which will be compared to the target.  \n",
      " \n",
      " \n",
      "5.2 Results              \n",
      " \n",
      "Graph for train loss which shows the deviation of error loss while t raining. A loss \n",
      "function, also known as a cost function, is a function that transfers an event or the \n",
      "values of one or more variables onto a real number that intuitively represents some\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Revised Summary:\n",
      "\n",
      "This project proposes a vision-based real-time hand gesture recognition technique for human-computer interaction. The proposed technique uses a Convolutional Neural Network (CNN) to extract features from hand gesture images and a Long Short-Term Memory (LSTM) to classify the hand gestures. The proposed technique is evaluated on the publicly available Sign Language MNIST dataset and achieves an accuracy of 93%.\n",
      "\n",
      "The proposed technique has several advantages over existing techniques. First, it is able to recognize hand gestures in real-time. Second, it is able to recognize hand gestures from different viewpoints. Third, it is able to recognize hand gestures that are partially occluded. Finally, the proposed technique is able to recognize hand gestures that are performed in different lighting conditions.\n",
      "\n",
      "The proposed technique can be used in a variety of applications, such as sign language recognition, human-computer interaction, and video surveillance.\n",
      "\n",
      "The context provides additional information about the challenges of vision-based hand gesture recognition. It also provides an overview of existing techniques for hand gesture recognition. This information can be used to improve the proposed technique and make it more robust to challenges such as variable lighting conditions, dynamic background, and skin color detection.\n",
      "\n",
      "In particular, the context provides information about the following challenges:\n",
      "\n",
      "* **Variable lighting conditions:** Hand gesture recognition algorithms are often sensitive to changes in lighting conditions. This can make it difficult to recognize hand gestures in different lighting conditions. The proposed technique uses a CNN to extract features from hand gesture images. CNNs are invariant to changes in lighting conditions, which makes them well-suited for hand gesture recognition in different lighting conditions.\n",
      "* **Dynamic background:** Hand gesture recognition algorithms are often sensitive to the background in which the hand gestures are performed. This can make it difficult to recognize hand gestures that are performed in front of a cluttered or dynamic background. The proposed technique uses a LSTM to classify hand gestures. LSTMs are able to learn long-term dependencies, which makes them well-suited for hand gesture recognition in front of a cluttered or dynamic background.\n",
      "* **Skin color detection:** Hand gesture recognition algorithms often require the user to wear a colored glove or data glove in order to track the hand gestures. This can be inconvenient for the user and can also make it difficult to recognize hand gestures in front of a colored background. The proposed technique does not require the user to wear a colored glove or data glove. This makes it more convenient for the user and also makes it easier to recognize hand gestures in front of a colored background.\n",
      "\n",
      "The proposed technique addresses these challenges by using a CNN to extract features from hand gesture images and a LSTM to classify hand gestures. This makes the proposed technique more robust to changes in lighting conditions, dynamic background, and skin color detection.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "\"cost\" connected with the event in mathematical optimization and decis ion theory. This \n",
      "is the initial data epochs vs error loss. An epoch is a word used in machine learning that \n",
      "refers to the number of passes the machine learning algorithm has made across the full \n",
      "training dataset. At 0 epoch, loss calculated is approx. 0.00 5. \n",
      " \n",
      "Figure 9. Train Loss  \n",
      " \n",
      "30 \n",
      " The graph below depicts the initial training accuracy, which indicates that the model was \n",
      "saved after a checkpoint. At 0 epoch, accuracy attained is approx 83%.As the training \n",
      "moves forward it changes to 98%.  \n",
      " \n",
      "Figure 10. Training Accuracy  \n",
      " \n",
      " \n",
      " \n",
      "31 \n",
      "  \n",
      "       \n",
      " \n",
      " \n",
      " \n",
      "32 \n",
      "  \n",
      " \n",
      " \n",
      ": \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "33 \n",
      " CHAPTER 6 – CONCLUSION AND FUTURE WORK  \n",
      " \n",
      " \n",
      "6.1 Conclusion  \n",
      " \n",
      "Using a CustomCNN classifier, the output is a fully trained model with a 93 percent \n",
      "accuracy. With a total of 1200 samples, the visuals used were in Indian Sign Language.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Revised Summary:\n",
      "\n",
      "This project proposes a vision-based real-time hand gesture recognition technique for human-computer interaction. The proposed technique uses a Convolutional Neural Network (CNN) to extract features from hand gesture images and a Long Short-Term Memory (LSTM) to classify the hand gestures. The proposed technique is evaluated on the publicly available Sign Language MNIST dataset and achieves an accuracy of 93%.\n",
      "\n",
      "The proposed technique has several advantages over existing techniques. First, it is able to recognize hand gestures in real-time. Second, it is able to recognize hand gestures from different viewpoints. Third, it is able to recognize hand gestures that are partially occluded. Finally, the proposed technique is able to recognize hand gestures that are performed in different lighting conditions.\n",
      "\n",
      "The proposed technique can be used in a variety of applications, such as sign language recognition, human-computer interaction, and video surveillance.\n",
      "\n",
      "The context provides additional information about the challenges of vision-based hand gesture recognition. It also provides an overview of existing techniques for hand gesture recognition. This information can be used to improve the proposed technique and make it more robust to challenges such as variable lighting conditions, dynamic background, and skin color detection.\n",
      "\n",
      "In particular, the context provides information about the following challenges:\n",
      "\n",
      "* **Variable lighting conditions:** Hand gesture recognition algorithms are often sensitive to changes in lighting conditions. This can make it difficult to recognize hand gestures in different lighting conditions. The proposed technique uses a CNN to extract features from hand gesture images. CNNs are invariant to changes in lighting conditions, which makes them well-suited for hand gesture recognition in different lighting conditions.\n",
      "* **Dynamic background:** Hand gesture recognition algorithms are often sensitive to the background in which the hand gestures are performed. This can make it difficult to recognize hand gestures that are performed in front of a cluttered or dynamic background. The proposed technique uses a LSTM to classify hand gestures. LSTMs are able to learn long-term dependencies, which makes them well-suited for hand gesture recognition in front of a cluttered or dynamic background.\n",
      "* **Skin color detection:** Hand gesture recognition algorithms often require the user to wear a colored glove or data glove in order to track the hand gestures. This can be inconvenient for the user and can also make it difficult to recognize hand gestures in front of a colored background. The proposed technique does not require the user to wear a colored glove or data glove. This makes it more convenient for the user and also makes it easier to recognize hand gestures in front of a colored background.\n",
      "\n",
      "The proposed technique addresses these challenges by using a CNN to extract features from hand gesture images and a LSTM to classify hand gestures. This makes the proposed technique more robust to changes in lighting conditions, dynamic background, and skin color detection.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "These photographs are organised into folders based on their meaning. The dataset was \n",
      "downloaded in jpeg format, then label binarized and pre -processed. We achieved a loss \n",
      "rate of roughly 0.10 after training the model. After that, the trained model was saved and \n",
      "reloaded. A script was used to enable the web camera. Finally, a hand gesture w as \n",
      "demonstrated, which was effectively detected.  \n",
      " \n",
      " \n",
      "6.2 Future Work  \n",
      " \n",
      "In the future, this might be modified into an application where a person practicing sign \n",
      "language can transmit live video to the other end, which can be understood by the \n",
      "person watching. Basically, when the video is sequenced, it will capture the main \n",
      "gesture which classifies as the sign language, and then the recognized sign is \n",
      "classified. The database can be modified by adding more images to it. This will lead to \n",
      "making the system more accurate in detecting everything.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "  34 \n",
      " REFERENCES  \n",
      " \n",
      " \n",
      "NumPy, Pandas, MatplotLib: GeekForGeeks: geekforgeeks.org\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Revised Summary:\n",
      "\n",
      "This project proposes a vision-based real-time hand gesture recognition technique for human-computer interaction. The proposed technique uses a Convolutional Neural Network (CNN) to extract features from hand gesture images and a Long Short-Term Memory (LSTM) to classify the hand gestures. The proposed technique is evaluated on the publicly available Sign Language MNIST dataset and achieves an accuracy of 93%.\n",
      "\n",
      "The proposed technique has several advantages over existing techniques. First, it is able to recognize hand gestures in real-time. Second, it is able to recognize hand gestures from different viewpoints. Third, it is able to recognize hand gestures that are partially occluded. Finally, the proposed technique is able to recognize hand gestures that are performed in different lighting conditions.\n",
      "\n",
      "The proposed technique can be used in a variety of applications, such as sign language recognition, human-computer interaction, and video surveillance.\n",
      "\n",
      "The context provides additional information about the challenges of vision-based hand gesture recognition. It also provides an overview of existing techniques for hand gesture recognition. This information can be used to improve the proposed technique and make it more robust to challenges such as variable lighting conditions, dynamic background, and skin color detection.\n",
      "\n",
      "In particular, the context provides information about the following challenges:\n",
      "\n",
      "* **Variable lighting conditions:** Hand gesture recognition algorithms are often sensitive to changes in lighting conditions. This can make it difficult to recognize hand gestures in different lighting conditions. The proposed technique uses a CNN to extract features from hand gesture images. CNNs are invariant to changes in lighting conditions, which makes them well-suited for hand gesture recognition in different lighting conditions.\n",
      "* **Dynamic background:** Hand gesture recognition algorithms are often sensitive to the background in which the hand gestures are performed. This can make it difficult to recognize hand gestures that are performed in front of a cluttered or dynamic background. The proposed technique uses a LSTM to classify hand gestures. LSTMs are able to learn long-term dependencies, which makes them well-suited for hand gesture recognition in front of a cluttered or dynamic background.\n",
      "* **Skin color detection:** Hand gesture recognition algorithms often require the user to wear a colored glove or data glove in order to track the hand gestures. This can be inconvenient for the user and can also make it difficult to recognize hand gestures in front of a colored background. The proposed technique does not require the user to wear a colored glove or data glove. This makes it more convenient for the user and also makes it easier to recognize hand gestures in front of a colored background.\n",
      "\n",
      "The proposed technique addresses these challenges by using a CNN to extract features from hand gesture images and a LSTM to classify hand gestures. This makes the proposed technique more robust to changes in lighting conditions, dynamic background, and skin color detection.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Python3: www.python.org/download/releases/3.0/  \n",
      " \n",
      "Machine Learning : towardsdatascience.com  \n",
      " \n",
      " \n",
      "[1] Study of vision -based hand gesture recognition using Indian sign language in \n",
      "INTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS \n",
      "VOL. 7, NO. 1, MARCH 2014.  \n",
      "[2] Real-time hand gesture recognition using finger segmentation in THE SCIENTIFIC \n",
      "WORLD JOURNAL (JAN 2014).  \n",
      "[3] Hand sign recognition from depth images with multi -scale density features for deaf -\n",
      "mute persons in International Conference on Computational Intelligence and Data \n",
      "Science (ICCIDS 2019).  \n",
      " \n",
      "[4] Indian sign language interpreter using image processing and machine learning in \n",
      "IOP Conference Series: Materials Science and Engineering 2020.  \n",
      " \n",
      "[5] Dynamic Hand Gesture Recognition: A Literature Review in IJERT 2012.  \n",
      " \n",
      "[6] Hand Gesture Recognition A Literature Review in IJSRD - International Journal for \n",
      "Scientific Research & Development| Vol. 8, Issue 2, 2020.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Revised Summary:\n",
      "\n",
      "This project proposes a vision-based real-time hand gesture recognition technique for human-computer interaction. The proposed technique uses a Convolutional Neural Network (CNN) to extract features from hand gesture images and a Long Short-Term Memory (LSTM) to classify the hand gestures. The proposed technique is evaluated on the publicly available Sign Language MNIST dataset and achieves an accuracy of 93%.\n",
      "\n",
      "The proposed technique has several advantages over existing techniques. First, it is able to recognize hand gestures in real-time. Second, it is able to recognize hand gestures from different viewpoints. Third, it is able to recognize hand gestures that are partially occluded. Finally, the proposed technique is able to recognize hand gestures that are performed in different lighting conditions.\n",
      "\n",
      "The proposed technique can be used in a variety of applications, such as sign language recognition, human-computer interaction, and video surveillance.\n",
      "\n",
      "The context provides additional information about the challenges of vision-based hand gesture recognition. It also provides an overview of existing techniques for hand gesture recognition. This information can be used to improve the proposed technique and make it more robust to challenges such as variable lighting conditions, dynamic background, and skin color detection.\n",
      "\n",
      "In particular, the context provides information about the following challenges:\n",
      "\n",
      "* **Variable lighting conditions:** Hand gesture recognition algorithms are often sensitive to changes in lighting conditions. This can make it difficult to recognize hand gestures in different lighting conditions. The proposed technique uses a CNN to extract features from hand gesture images. CNNs are invariant to changes in lighting conditions, which makes them well-suited for hand gesture recognition in different lighting conditions.\n",
      "* **Dynamic background:** Hand gesture recognition algorithms are often sensitive to the background in which the hand gestures are performed. This can make it difficult to recognize hand gestures that are performed in front of a cluttered or dynamic background. The proposed technique uses a LSTM to classify hand gestures. LSTMs are able to learn long-term dependencies, which makes them well-suited for hand gesture recognition in front of a cluttered or dynamic background.\n",
      "* **Skin color detection:** Hand gesture recognition algorithms often require the user to wear a colored glove or data glove in order to track the hand gestures. This can be inconvenient for the user and can also make it difficult to recognize hand gestures in front of a colored background. The proposed technique does not require the user to wear a colored glove or data glove. This makes it more convenient for the user and also makes it easier to recognize hand gestures in front of a colored background.\n",
      "\n",
      "The proposed technique addresses these challenges by using a CNN to extract features from hand gesture images and a LSTM to classify hand gestures. This makes the proposed technique more robust to changes in lighting conditions, dynamic background, and skin color detection.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "[7] Tracking of dynamic gesture fingertips position in a video sequence in Archives of \n",
      "Control Sciences Volume 30(LX VI), 2020.  \n",
      "  [8] Visual tracking utilizing robust complementary learner and adaptive refiner in \n",
      "Science   Direct,10th May 2017  35 \n",
      " [9] Tracking of dynamic gesture fingertips position in a video sequence in Archives of \n",
      "Control Sciences Volume 30(LXVI), 2020.  \n",
      "[10] Visual tracking utilizing robust complementary learner and adaptive refiner in \n",
      "Science Direct,10th May 2017.  \n",
      "[11] Sign Language Recognition Using Image Processing, Research Gate \n",
      "2017.Development of the Recognition System of Japanese Sign Language Using  3D \n",
      "Image Sensor (HCI International 2013).\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm = llm,\n",
    "    chain_type = 'refine',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "output_summary = chain.run(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
